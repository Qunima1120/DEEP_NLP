{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import spacy \n",
    "import numpy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 5.68 s, total: 58.5 s\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#nlp=spacy.load('en_vectors_web_lg')\n",
    "%time nlp=spacy.load('./wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRecognizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner = EntityRecognizer(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            ##rever to word vector\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take SRS descripitions for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%%time \\ninS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\\nsrsS=pd.read_pickle(inS_dir)\\nsrsS=pd.Series(data=srsS.values,index=pd.MultiIndex.from_arrays([srsS.index.get_level_values(0),\\n                                                            srsS.index.get_level_values(1).str.lower()]) )\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%%time \n",
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\n",
    "srsS=pd.read_pickle(inS_dir)\n",
    "srsS=pd.Series(data=srsS.values,index=pd.MultiIndex.from_arrays([srsS.index.get_level_values(0),\n",
    "                                                            srsS.index.get_level_values(1).str.lower()]) )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\\n\\ntechnical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\\n\\nnoDupSampleS=technical_meta_data_df.groupby(['Study']).head(n=1)['Sample']\\n\\nspecie_m=srsS.isin(['Mus musculus','Homo sapiens'])\\n\\nattrib_m=srsS.index.get_level_values(1)=='scientific_name'\\n\\nmySpecieSrs=srsS[specie_m&attrib_m].index.get_level_values(0).unique()\\n\\nspecie_srs_m=srsS.index.get_level_values(0).isin(mySpecieSrs)\\n\\nattrib_a=srsS.index.get_level_values(1)\\nattrib_m=attrib_a=='description'\\n\\n\\noneInStudy_m=srsS.index.get_level_values(0).isin(noDupSampleS.values)\\n\\nsrsS_sub=srsS[attrib_m&specie_srs_m&oneInStudy_m].drop_duplicates()\\n\\n#make sure the code doesn't sample from outliers\\n#20 words https://www.ijcai.org/proceedings/2017/0578.pdf\\nwordCountS=srsS_sub.str.count(' ')\\nlem_m=(wordCountS<=60)&(wordCountS>=10)\\nsrsS_sub=srsS_sub[lem_m]\\n\\ninTestStrS=srsS_sub.sample(n=100,random_state=0)\\n\\n### reload model\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\n",
    "\n",
    "technical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\n",
    "\n",
    "noDupSampleS=technical_meta_data_df.groupby(['Study']).head(n=1)['Sample']\n",
    "\n",
    "specie_m=srsS.isin(['Mus musculus','Homo sapiens'])\n",
    "\n",
    "attrib_m=srsS.index.get_level_values(1)=='scientific_name'\n",
    "\n",
    "mySpecieSrs=srsS[specie_m&attrib_m].index.get_level_values(0).unique()\n",
    "\n",
    "specie_srs_m=srsS.index.get_level_values(0).isin(mySpecieSrs)\n",
    "\n",
    "attrib_a=srsS.index.get_level_values(1)\n",
    "attrib_m=attrib_a=='description'\n",
    "\n",
    "\n",
    "oneInStudy_m=srsS.index.get_level_values(0).isin(noDupSampleS.values)\n",
    "\n",
    "srsS_sub=srsS[attrib_m&specie_srs_m&oneInStudy_m].drop_duplicates()\n",
    "\n",
    "#make sure the code doesn't sample from outliers\n",
    "#20 words https://www.ijcai.org/proceedings/2017/0578.pdf\n",
    "wordCountS=srsS_sub.str.count(' ')\n",
    "lem_m=(wordCountS<=60)&(wordCountS>=10)\n",
    "srsS_sub=srsS_sub[lem_m]\n",
    "\n",
    "inTestStrS=srsS_sub.sample(n=100,random_state=0)\n",
    "\n",
    "### reload model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 7.24 s, total: 28.4 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = numpy.load('./model/classes.npy')\n",
    "\n",
    "%time model=load_model('./model/lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTestStrS=pd.read_pickle('./Data/validation_description.pickle').head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inTestStrS.str.replace('[0-9 ]{2,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get baseline empty state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_docs = list(nlp.pipe(' '))\n",
    "val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "\n",
    "emptyState=model.predict_proba(val_X)[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate NER score for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopWords=stopwords.words('english')\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraseMax=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inTestStrS.str.split('[;.]',expand=True).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "myML=[]\n",
    "myKeyL=[]\n",
    "for i_th,sent in enumerate(tqdm(inTestStrS)):\n",
    "    sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "    tokens=re.split(pattern=' ',string=sent)\n",
    "    #\n",
    "    tokens=list(filter(lambda token:(token!='') and (token not in stopWords)  ,tokens))\n",
    "    sent=' '.join(tokens)\n",
    "    ###keep track of each token\n",
    "    scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "    #for n_gram in range(1,len(tokens)+1):\n",
    "    myNMax=min( [len(tokens),phraseMax])\n",
    "    #print (myNMax)\n",
    "    for n_gram in range(1,myNMax+1):\n",
    "        \n",
    "        grams=list(map(lambda L:\" \".join(L),list(ngrams(tokens,n_gram))))\n",
    "        val_docs = list(nlp.pipe(grams))\n",
    "        val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "        predictM=model.predict_proba(val_X)\n",
    "        tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "        empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.1\n",
    "        moreThanTwoValToken_mask=(val_X!=0).sum(axis=1)>=2\n",
    "        tmpDf[empty_mask&moreThanTwoValToken_mask]=0\n",
    "        \n",
    "        for i,gram in enumerate(tmpDf.index): #i: track the each token position\n",
    "            #for j,one_gram in enumerate(gram.split(' ')):\n",
    "            i_end=i+n_gram+1\n",
    "            textBefore=\" \".join(tokens[:i]) + ('' if i==0 else ' ')\n",
    "            start_char_pos=len(textBefore)\n",
    "            myKeyL.append((i_th,sent,n_gram,i,i_end,gram,start_char_pos)) \n",
    "            myML.append(tmpDf.iloc[i])\n",
    "                #(tmpDf.iloc[i])\n",
    "        #tmpDf2=tmpDf[(tmpDf>=0.5).any(axis=1)]\n",
    "        \n",
    "\n",
    "        #update score in table by \n",
    "        #for i,gram in enumerate(tmpDf.index):# for each n-gram, #for each word add the score\n",
    "        #        for j,one_gram in enumerate(gram.split(' ')): #check the score of each split compared to current ones\n",
    "        #                scoreDf.iloc[i+j]=scoreDf.iloc[i+j]+(tmpDf.iloc[i])\n",
    "        #                #scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n",
    "        \n",
    "    #if i_th>10:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_doc=ner.pipe(inTestStrS.iloc[:2].values)\n",
    "#list(tmp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textBefore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp=myKeyL[2]\n",
    "#tmp[1][6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpDf.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#tmpDf.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thresholdS=pd.Series(\\n   {'SCIENTIFIC_NAME': 0.26689231395721436,\\n 'cell type': 0.10027739964425564,\\n 'disease': 0.15823280811309814,\\n 'genotype': 0.09954400360584259,\\n 'geo_loc_name': 0.6163255572319031,\\n 'sex': 0.6458048224449158}\\n)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"thresholdS=pd.Series(\n",
    "   {'SCIENTIFIC_NAME': 0.26689231395721436,\n",
    " 'cell type': 0.10027739964425564,\n",
    " 'disease': 0.15823280811309814,\n",
    " 'genotype': 0.09954400360584259,\n",
    " 'geo_loc_name': 0.6163255572319031,\n",
    " 'sex': 0.6458048224449158}\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf=pd.concat(myML,keys=myKeyL,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf.index.names=['i_thSrs','orig_text','n','i','i_end','token','ith_char_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf_sub=mergedDf[mergedDf.index.get_level_values('n')>=n_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_m=mergedDf_sub.max(axis=1)>0.2\n",
    "m_val=((mergedDf_sub>0.05).sum(axis=1)==1)&max_m&(~mergedDf_sub.index.get_level_values('token').str.contains('[0-9 ]+ [0-9 ]+'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=pd.DataFrame({'predicted':mergedDf_sub[m_val].idxmax(axis=1),'score':mergedDf_sub[m_val].max(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "scoreSortedDf=tmpDf[m_val].sort_values(['orig_text','i','score'],ascending=False).reset_index(\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all overlapping regions\n",
    "v=scoreSortedDf.copy()\n",
    "scoreSortedDf=scoreSortedDf.assign(OverlapGroup=(len(inTestStrS)*v.i_thSrs+ \n",
    "                                          (v.i_end - v.i.shift(-1)).shift().lt(0).cumsum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_thSrs</th>\n",
       "      <th>orig_text</th>\n",
       "      <th>n</th>\n",
       "      <th>i</th>\n",
       "      <th>i_end</th>\n",
       "      <th>token</th>\n",
       "      <th>ith_char_pos</th>\n",
       "      <th>predicted</th>\n",
       "      <th>score</th>\n",
       "      <th>OverlapGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84</td>\n",
       "      <td>tdTomato cardiomyocytes Mus musculus CAG creER...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>mapping identifies</td>\n",
       "      <td>106</td>\n",
       "      <td>genotype</td>\n",
       "      <td>0.818595</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>tdTomato cardiomyocytes Mus musculus CAG creER...</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>fate mapping identifies</td>\n",
       "      <td>101</td>\n",
       "      <td>genotype</td>\n",
       "      <td>0.582801</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84</td>\n",
       "      <td>tdTomato cardiomyocytes Mus musculus CAG creER...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>Hypoxia fate</td>\n",
       "      <td>93</td>\n",
       "      <td>treatment</td>\n",
       "      <td>0.203004</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>tdTomato cardiomyocytes Mus musculus CAG creER...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>tdTomato transgenic</td>\n",
       "      <td>58</td>\n",
       "      <td>genotype</td>\n",
       "      <td>0.660796</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>tdTomato cardiomyocytes Mus musculus CAG creER...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>R26R tdTomato</td>\n",
       "      <td>53</td>\n",
       "      <td>genotype</td>\n",
       "      <td>0.764524</td>\n",
       "      <td>8400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i_thSrs                                          orig_text  n   i  i_end  \\\n",
       "0       84  tdTomato cardiomyocytes Mus musculus CAG creER...  2  14     17   \n",
       "1       84  tdTomato cardiomyocytes Mus musculus CAG creER...  3  13     17   \n",
       "2       84  tdTomato cardiomyocytes Mus musculus CAG creER...  2  12     15   \n",
       "3       84  tdTomato cardiomyocytes Mus musculus CAG creER...  2   8     11   \n",
       "4       84  tdTomato cardiomyocytes Mus musculus CAG creER...  2   7     10   \n",
       "\n",
       "                     token  ith_char_pos  predicted     score  OverlapGroup  \n",
       "0       mapping identifies           106   genotype  0.818595          8400  \n",
       "1  fate mapping identifies           101   genotype  0.582801          8400  \n",
       "2             Hypoxia fate            93  treatment  0.203004          8400  \n",
       "3      tdTomato transgenic            58   genotype  0.660796          8400  \n",
       "4            R26R tdTomato            53   genotype  0.764524          8400  "
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreSortedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf=scoreSortedDf.sort_values(['OverlapGroup','score'],ascending=False).drop_duplicates(['OverlapGroup','predicted']\n",
    "                                                                                   ).sort_values('orig_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['token_len']=hitDf['token'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['recovered_txt']=hitDf.apply(\n",
    "    lambda tmpS2:tmpS2.loc['orig_text'][tmpS2.loc['ith_char_pos']:(tmpS2.loc['ith_char_pos']+tmpS2.loc['token_len'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is it extraction:  True\n"
     ]
    }
   ],
   "source": [
    "print ('is it extraction: ',(hitDf.recovered_txt==hitDf.token).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}\n",
    "\\\n",
    "Content contains input text, annotation has the labeled content, extras is for some extra columns that you want to show with each row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClassToId={}\n",
    "for myClass in le.classes_:\n",
    "    myClassToId[myClass]=nlp.vocab.strings.add(myClass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "classToSpacyId=pd.Series(myClassToId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDisplayHitDf=hitDf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDisplayHitDf['predicted_entity_id']=classToSpacyId.loc[inDisplayHitDf['predicted']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_textL=sorted(inDisplayHitDf['orig_text'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text=orig_textL[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocols Hela cells treatment NC ASO snoRD50a ASOs treated concentration 50 nM used RNAi max ASO transfection harvest cells 2 days transfection RNAs recovered Fip1 antibodies ligated onto 3 adaptor RT reactions carried converting RNAs cDNA cDNAs circularized digested PCR amplified PCR product sent sequencing\n"
     ]
    }
   ],
   "source": [
    "print (orig_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf_groupby=inDisplayHitDf.groupby('orig_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "inRecordDisplayDf=hitDf_groupby.get_group(orig_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_text=orig_textS\n",
    "doc=nlp(orig_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17998780750743588112, 7, 10)\n",
      "(10400321950887182855, 19, 21)\n",
      "(188603156775490070, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "for _,tmpS in inRecordDisplayDf.iterrows():\n",
    "    EVENT=tmpS['predicted_entity_id']\n",
    "    entity=(EVENT,tmpS['i'],tmpS['i_end']-1)# this the correct one\n",
    "    print (entity)\n",
    "    doc.ents+=(entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SCIENTIFIC_NAME     8276535918164649739\n",
       "age                10400321950887182855\n",
       "cell type            188603156775490070\n",
       "disease             5781322434961390358\n",
       "genotype           18166236633804328029\n",
       "geo_loc_name       12368356759224854017\n",
       "treatment          17998780750743588112\n",
       "dtype: uint64"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classToSpacyId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_palette = sns.color_palette()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABLCAYAAABz9YPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAArBJREFUeJzt3aFqlmEYxvH7FedWFkQHDkQZw+aCuKqIJyFosO0UPASbWCxiswkegh6BNkUwDJsMJsKSYfJYLBY/xnh49r3X79cGT7jutD+8g02ttQIASHVu9AAAgJHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHOL3owTdNeVe1VVU0ra7dXLl3tPmqUnWl/9ISuPq9eGD2hm+3voxf0dbR+bfSErtrvg9ETurq4emX0hK4Op6PRE7pZX/8xekJX+7U9ekJXx1+/HLbWNha9m07y7zhWN2+0zcfPTzXsLPu29nD0hK52tub7C/XN0+PRE7p6f+/F6Ald/fr5bPSErh5sPRk9oatXa+9GT+jmzt3Xoyd09Wh6O3pCVwf3b31sre0ueuczGQAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANGm1tr/H0zTXlXt/f3xZlV96j1qoMtVdTh6RCdzvq3KfcvOfctrzrdVuW/ZXW+tbSx6tDCG/nk8TR9aa7unmnWGzfm+Od9W5b5l577lNefbqtyXwmcyACCaGAIAop00hl52WXF2zPm+Od9W5b5l577lNefbqtwX4UR/MwQAMDc+kwEA0cQQABBNDAEA0cQQABBNDAEA0f4Az7tyiUpEylQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_palette\n",
    "sns.palplot(current_palette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Protocols \n",
       "<mark class=\"entity\" style=\"background: #2ca02c; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Hela cells\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">cell type</span>\n",
       "</mark>\n",
       " treatment NC ASO snoRD50a \n",
       "<mark class=\"entity\" style=\"background: #e377c2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    ASOs treated concentration\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " 50 nM used RNAi max ASO transfection harvest cells \n",
       "<mark class=\"entity\" style=\"background: #ff7f0e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">age</span>\n",
       "</mark>\n",
       " transfection RNAs recovered Fip1 antibodies ligated onto 3 adaptor RT reactions carried converting RNAs cDNA cDNAs circularized digested PCR amplified PCR product sent sequencing</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = {ent.upper():matplotlib.colors.to_hex(myColor)  for myColor,ent in zip(current_palette,classToSpacyId.index)}\n",
    "\n",
    "options = {'ents': classToSpacyId.index.str.upper(),\n",
    "           'colors': colors}\n",
    "\n",
    "spacy.displacy.render(doc, style='ent',jupyter=True,options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=spacy.displacy.render([doc,doc], style='ent',page=True,options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/displacy.html','w') as f:\n",
    "    f.write(html)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/cellardata/users/btsui/DEEP_NLP/NLP_spacy/./Data/displacy.html\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD/./Data/displacy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdasdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-804-09a9202377d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdasdasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasdasd' is not defined"
     ]
    }
   ],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfasf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-757-9f9b4d966f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdfasf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfasf' is not defined"
     ]
    }
   ],
   "source": [
    "asdfasf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerName='cell type'\n",
    "nlp.vocab.strings.add(nerName)\n",
    "EVENT = nlp.vocab.strings[nerName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188603156775490070"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"t cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use \n",
    "entity=(EVENT,0,len(u\"t cell\"))\n",
    "doc.ents+=(entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='ent',jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc\n",
    "    doc.ents += (entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToke.label=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents+=tuple(['cell type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span=doc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span.label='cell type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export to dataturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myF(tmpS1):\n",
    "    return {\"label\":[tmpS1.loc['predicted']],\"points\":{'start':tmpS1.loc['i'],\n",
    "     'end':tmpS1.loc['i_end']-1,'text':tmpS1.loc['token']},\n",
    "\n",
    "        }\n",
    "\n",
    "inAnnotDf=hitDf.copy()\n",
    "myL=[]\n",
    "for text,subDf in inAnnotDf.groupby(['orig_text']):\n",
    "    oneAnnotatedLine={\"content\":text,'annotation':list(subDf.apply(myF,axis=1)),\n",
    "                                \"extras\":None,\n",
    "        \"metadata\":{\"first_done_at\":1535058971000,\n",
    "                                     \"last_updated_at\":1535058971000,\"sec_taken\":0,\n",
    "                                \"last_updated_by\":\"EEOBDlEO48T6gzo0KHvT0IkqZnn2\"}}\n",
    "    myL+=[json.dumps(oneAnnotatedLine)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( './Data/validation.ngram.json.txt','w')as f:\n",
    "    f.write(\"\\n\".join(myL[:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Data/validation.ngram.json.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\",\".join(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneAnnotatedLine['annotation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDict={\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}\n",
    "exampleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}\n",
    "#myL=[]\n",
    "#{\"content\":text,\"annotation\":}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inAnnotDf.iloc[:1].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['i_thSrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['i_thSrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf.sort_values(['i_thSrs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNameL=list(VC.index.names)\n",
    "myNameL[-1]='Predicted_NE'\n",
    "VC.index.names=myNameL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for each n-gram, there is the same start site, use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.drop_duplicates(['orig_text','Predicted_NE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDf\n",
    "#take on any length, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=(mergedDf>thresholdS).loc['ESCs WT replicate1 mRNA Mad2l2'].stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf.columns=['n','n-gram','attrib','passThreshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpDf['n-gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf[tmpDf.passThreshold]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpS[tmpS].groupby(level=[1,2]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf2.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf=pd.concat(myML,keys=list(inTestStrS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_token_m=mergedDf.index.get_level_values(0).str.contains('^\\d+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf=mergedDf[~numeric_token_m].copy()#.loc[:,mergedDf.columns!='age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.800000\n",
    "mergedSubDf['undetected']=threshold\n",
    "#noAmbigM=(mergedSubDf>=threshold).sum(axis=1)>=1 #this mark screw up the confoudning boundary \n",
    "mergedSubDf.loc[:,'undetected']=threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf=mergedSubDf.idxmax(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf.index.names=['Freetext','Token']\n",
    "predDf['token_numeric']=predDf.index.get_level_values('Token').str.contains('^\\d+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel=pd.ExcelWriter('./Results/for_curation.xlsx')\n",
    "predDf[~predDf['token_numeric']].to_excel(excel)\n",
    "excel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf[~predDf['token_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Results/for_curation.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Results/for_curation.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf[~predDf['token_numeric']].to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm tmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with duplicated states\n",
    "#emptyStat=np.array([0.42332533, 0.4360587 , 0.61020947, 0.42082471, 0.4110575 ,\n",
    "#       0.42533568, 0.47932082])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyState=mergedSubDf.groupby(mergedSubDf.columns.tolist(),as_index=False).size().sort_values().index[-1]\n",
    "#emptyStat=np.array([0.42332533, 0.4360587 , 0.61020947, 0.42082471, 0.4110575 ,\n",
    "#       0.42533568, 0.47932082])\n",
    "print (emptyState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noPredM=((mergedSubDf-emptyState).abs()<0.1).all(axis=1)\n",
    "mergedSubDf[(~noPredM)&(mergedSubDf>0.5).sum(axis=1)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good examples: HAP1 LMTK3-KO cells, stimulated with WNT3, replicate R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf.loc['HAP1 CCK4-KO cells, stimulated with RESV, replicate R1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf[].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=scoreDf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##among the ones with >0.5, take the ones that are unique\n",
    "sns.heatmap(data=(scoreDf>0.6).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=scoreDf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### among the ones with clear boundry, it can classify well. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "argue that it can salvage the data correctly. Among those sentences, \n",
    "\n",
    "take >0.5 as boundary, run top 10000 sentences \n",
    "\"\"\"\n",
    "scoreDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent='Human histone H3 di-methylated at lysine 4 (H3K4me2) in human blood CD4+ T cells, targeted using Abcam antibody ab7766'#inTestStrS.iloc[5]\n",
    "sent='RNA-seq of total RNA from Z/Edn2; Six3-Cre mouse retina\t'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "#print (sent)\n",
    "tokens=re.split(pattern=' ',string=sent)\n",
    "s=tokens\n",
    "#print (tokens)\n",
    "scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "#for n_gram in range(1,len(tokens)+1):\n",
    "#for n_gram in range(1,len(tokens)):\n",
    "n_gram=8\n",
    "grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "#print (grams)\n",
    "val_docs = list(nlp.pipe(grams))\n",
    "val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "tmpDf=pd.DataFrame(data=model.predict_proba(val_X),columns=le.classes_,index=grams)\n",
    "#tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "tmpDf[empty_mask]=0\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(3,2.5*4))\n",
    "sns.heatmap(tmpDf,annot=True,ax=ax,vmin=0,vmax=1.0,fmt='.2f',cbar=None)\n",
    "#ax.set_xticklabels([])\n",
    "\"\"\"break\n",
    "\n",
    "#each n gram only advange \n",
    "for i,gram in enumerate(tmpDf.index):# for ec\n",
    "    for j,one_gram in enumerate(gram.split(' ')):\n",
    "        scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each one, makes a prediction on the term, to see what it is supposed to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "#print (sent)\n",
    "tokens=re.split(pattern=' ',string=sent)\n",
    "s=tokens\n",
    "#print (tokens)\n",
    "scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "#for n_gram in range(1,len(tokens)+1):\n",
    "for n_gram in range(1,len(tokens)):\n",
    "    grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "    #print (grams)\n",
    "    val_docs = list(nlp.pipe(grams))\n",
    "    val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "    predictM=model.predict_proba(val_X)\n",
    "\n",
    "    tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "    empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "    tmpDf[empty_mask]=0\n",
    "\n",
    "    \"\"\"\n",
    "    each n gram only advange \n",
    "    \"\"\"\n",
    "    for i,gram in enumerate(tmpDf.index):# for ec\n",
    "        for j,one_gram in enumerate(gram.split(' ')):\n",
    "            scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.2\n",
    "scoreDf[scoreDf<=threshold]=0\n",
    "scoreDf['undetected']=threshold\n",
    "\n",
    "scoreDf.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scoreDf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scoreDf.T,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexS=srsS[srsS.index.get_level_values(1)=='sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSubSrsS1=srsS[srsS.str.contains('rna',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSubSrsS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
