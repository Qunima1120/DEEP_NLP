{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import spacy \n",
    "import numpy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 5.68 s, total: 58.5 s\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#nlp=spacy.load('en_vectors_web_lg')\n",
    "%time nlp=spacy.load('./wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRecognizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner = EntityRecognizer(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            ##rever to word vector\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take SRS descripitions for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%%time \\ninS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\\nsrsS=pd.read_pickle(inS_dir)\\nsrsS=pd.Series(data=srsS.values,index=pd.MultiIndex.from_arrays([srsS.index.get_level_values(0),\\n                                                            srsS.index.get_level_values(1).str.lower()]) )\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%%time \n",
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\n",
    "srsS=pd.read_pickle(inS_dir)\n",
    "srsS=pd.Series(data=srsS.values,index=pd.MultiIndex.from_arrays([srsS.index.get_level_values(0),\n",
    "                                                            srsS.index.get_level_values(1).str.lower()]) )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\\n\\ntechnical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\\n\\nnoDupSampleS=technical_meta_data_df.groupby(['Study']).head(n=1)['Sample']\\n\\nspecie_m=srsS.isin(['Mus musculus','Homo sapiens'])\\n\\nattrib_m=srsS.index.get_level_values(1)=='scientific_name'\\n\\nmySpecieSrs=srsS[specie_m&attrib_m].index.get_level_values(0).unique()\\n\\nspecie_srs_m=srsS.index.get_level_values(0).isin(mySpecieSrs)\\n\\nattrib_a=srsS.index.get_level_values(1)\\nattrib_m=attrib_a=='description'\\n\\n\\noneInStudy_m=srsS.index.get_level_values(0).isin(noDupSampleS.values)\\n\\nsrsS_sub=srsS[attrib_m&specie_srs_m&oneInStudy_m].drop_duplicates()\\n\\n#make sure the code doesn't sample from outliers\\n#20 words https://www.ijcai.org/proceedings/2017/0578.pdf\\nwordCountS=srsS_sub.str.count(' ')\\nlem_m=(wordCountS<=60)&(wordCountS>=10)\\nsrsS_sub=srsS_sub[lem_m]\\n\\ninTestStrS=srsS_sub.sample(n=100,random_state=0)\\n\\n### reload model\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\n",
    "\n",
    "technical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\n",
    "\n",
    "noDupSampleS=technical_meta_data_df.groupby(['Study']).head(n=1)['Sample']\n",
    "\n",
    "specie_m=srsS.isin(['Mus musculus','Homo sapiens'])\n",
    "\n",
    "attrib_m=srsS.index.get_level_values(1)=='scientific_name'\n",
    "\n",
    "mySpecieSrs=srsS[specie_m&attrib_m].index.get_level_values(0).unique()\n",
    "\n",
    "specie_srs_m=srsS.index.get_level_values(0).isin(mySpecieSrs)\n",
    "\n",
    "attrib_a=srsS.index.get_level_values(1)\n",
    "attrib_m=attrib_a=='description'\n",
    "\n",
    "\n",
    "oneInStudy_m=srsS.index.get_level_values(0).isin(noDupSampleS.values)\n",
    "\n",
    "srsS_sub=srsS[attrib_m&specie_srs_m&oneInStudy_m].drop_duplicates()\n",
    "\n",
    "#make sure the code doesn't sample from outliers\n",
    "#20 words https://www.ijcai.org/proceedings/2017/0578.pdf\n",
    "wordCountS=srsS_sub.str.count(' ')\n",
    "lem_m=(wordCountS<=60)&(wordCountS>=10)\n",
    "srsS_sub=srsS_sub[lem_m]\n",
    "\n",
    "inTestStrS=srsS_sub.sample(n=100,random_state=0)\n",
    "\n",
    "### reload model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 7.24 s, total: 28.4 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = numpy.load('./model/classes.npy')\n",
    "%time model=load_model('./model/lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin.gz\r\n",
      "NCBI_harmonized_name_df.tsv\r\n",
      "NCIT_table.pickle\r\n",
      "PMC-ids.csv.gz\r\n",
      "PMC-ids.csv.gz.1\r\n",
      "PubMed-w2v.bin\r\n",
      "PubMed-w2v.bin.1\r\n",
      "displacy.html\r\n",
      "hmaps_batch\r\n",
      "medtag\r\n",
      "medtag.tar.gz\r\n",
      "pmc_word2_vec.gensim\r\n",
      "pmc_word2_vec.gensim.trainables.syn1neg.npy\r\n",
      "pmc_word2_vec.gensim.wv.vectors.npy\r\n",
      "pmc_word2_vec.pandas.threshold.10.pickle\r\n",
      "pmc_word2_vec.pandas.threshold.100.pickle\r\n",
      "pmc_word2_vec.pandas.threshold.200.pickle\r\n",
      "pmc_word2_vec.txt\r\n",
      "spacy_bio_nlp\r\n",
      "text.txt\r\n",
      "validation.ngram.json.txt\r\n",
      "validation_cell line.pickle\r\n",
      "validation_cell line.xlsx\r\n",
      "validation_description.pickle\r\n",
      "validation_description.xlsx\r\n",
      "validation_lines.txt\r\n",
      "validation_title.pickle\r\n",
      "validation_title.xlsx\r\n",
      "validation_{}.txt\r\n",
      "wikipedia-pubmed-and-PMC-w2v.bin\r\n",
      "wikipedia-pubmed-and-PMC-w2v.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedValidAttrib='description'\n",
    "inTestStrFlatS=pd.read_pickle('./Data/validation_{}.pickle'.format(selectedValidAttrib)).head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1493,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inTestStrS.str.replace('[0-9 ]{2,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get baseline empty state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1494,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_docs = list(nlp.pipe(' '))\n",
    "val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "\n",
    "emptyState=model.predict_proba(val_X)[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate NER score for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1495,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopWords=stopwords.words('english')\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraseMax=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1498,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTestStrS=inTestStrFlatS.str.split('[;.,]',expand=True).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1499,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTestStrS=inTestStrS.str.replace('\\s+',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/577 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 3/577 [00:00<00:24, 23.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 5/577 [00:00<00:28, 19.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 7/577 [00:00<00:32, 17.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 11/577 [00:00<00:27, 20.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 15/577 [00:00<00:25, 21.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 17/577 [00:00<00:26, 21.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 20/577 [00:00<00:26, 21.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 22/577 [00:01<00:28, 19.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 25/577 [00:01<00:27, 20.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 27/577 [00:01<00:27, 20.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 30/577 [00:01<00:29, 18.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 32/577 [00:01<00:29, 18.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 37/577 [00:01<00:27, 19.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 40/577 [00:02<00:27, 19.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 43/577 [00:02<00:26, 19.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 46/577 [00:02<00:26, 19.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 49/577 [00:02<00:26, 19.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 52/577 [00:02<00:25, 20.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 56/577 [00:02<00:24, 20.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 59/577 [00:02<00:24, 21.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 62/577 [00:02<00:24, 20.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|█▏        | 65/577 [00:03<00:24, 21.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 68/577 [00:03<00:24, 21.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 71/577 [00:03<00:24, 20.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 74/577 [00:03<00:24, 20.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▎        | 78/577 [00:03<00:23, 21.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 81/577 [00:03<00:23, 20.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 85/577 [00:03<00:23, 21.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 88/577 [00:04<00:23, 21.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 91/577 [00:04<00:23, 20.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▋        | 95/577 [00:04<00:22, 21.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 99/577 [00:04<00:22, 21.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 102/577 [00:04<00:22, 21.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 105/577 [00:05<00:22, 20.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▊        | 108/577 [00:05<00:22, 20.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 111/577 [00:05<00:22, 21.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|█▉        | 114/577 [00:05<00:21, 21.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 117/577 [00:05<00:21, 21.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 120/577 [00:05<00:21, 21.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|██▏       | 123/577 [00:05<00:21, 21.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 128/577 [00:05<00:20, 21.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 131/577 [00:06<00:20, 21.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 134/577 [00:06<00:20, 21.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▎       | 137/577 [00:06<00:20, 21.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 140/577 [00:06<00:20, 21.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▍       | 143/577 [00:06<00:20, 21.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 146/577 [00:06<00:20, 21.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 149/577 [00:06<00:20, 21.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 152/577 [00:07<00:19, 21.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 156/577 [00:07<00:19, 21.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 159/577 [00:07<00:19, 21.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 162/577 [00:07<00:19, 21.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 165/577 [00:07<00:19, 21.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 168/577 [00:07<00:19, 21.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 171/577 [00:07<00:18, 21.43it/s]\u001b[A\u001b[A\u001b[A\u001b[AException ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 883, in __del__\n",
      "    self.close()\n",
      "  File \"/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 1088, in close\n",
      "    self._decr_instances(self)\n",
      "  File \"/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 439, in _decr_instances\n",
      "    cls._instances.remove(instance)\n",
      "  File \"/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x2b5385f65ef8; to 'tqdm' at 0x2b53834d0ef0>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 175/577 [00:08<00:20, 19.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 178/577 [00:08<00:20, 19.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 181/577 [00:09<00:19, 19.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 185/577 [00:09<00:19, 19.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 188/577 [00:09<00:19, 19.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 191/577 [00:09<00:19, 19.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 193/577 [00:09<00:19, 19.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 196/577 [00:09<00:19, 19.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 198/577 [00:10<00:19, 19.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 201/577 [00:10<00:18, 19.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 204/577 [00:10<00:18, 19.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 207/577 [00:10<00:18, 19.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 210/577 [00:10<00:18, 19.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 213/577 [00:10<00:18, 19.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 215/577 [00:10<00:18, 19.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 218/577 [00:11<00:18, 19.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 222/577 [00:11<00:17, 19.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 226/577 [00:11<00:17, 20.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███▉      | 229/577 [00:11<00:17, 19.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 232/577 [00:11<00:17, 19.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 234/577 [00:11<00:17, 19.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 237/577 [00:11<00:17, 19.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 239/577 [00:12<00:17, 19.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 243/577 [00:12<00:16, 19.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 246/577 [00:12<00:16, 19.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 248/577 [00:12<00:16, 19.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▎     | 251/577 [00:12<00:16, 19.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 253/577 [00:12<00:16, 19.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 257/577 [00:12<00:16, 19.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 260/577 [00:13<00:15, 19.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 263/577 [00:13<00:15, 19.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 266/577 [00:13<00:15, 19.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 269/577 [00:13<00:15, 19.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 272/577 [00:13<00:15, 19.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 275/577 [00:13<00:15, 19.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 278/577 [00:13<00:14, 20.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▊     | 281/577 [00:14<00:14, 19.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 284/577 [00:14<00:14, 19.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 287/577 [00:14<00:14, 19.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 289/577 [00:14<00:14, 19.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 291/577 [00:14<00:14, 19.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 293/577 [00:15<00:14, 19.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 295/577 [00:15<00:14, 19.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 298/577 [00:15<00:14, 19.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 300/577 [00:15<00:14, 19.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 303/577 [00:15<00:14, 19.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 306/577 [00:15<00:13, 19.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▎    | 310/577 [00:15<00:13, 19.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 313/577 [00:15<00:13, 19.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▍    | 316/577 [00:16<00:13, 19.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 319/577 [00:16<00:13, 19.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 323/577 [00:16<00:12, 19.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 326/577 [00:16<00:12, 19.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 331/577 [00:16<00:12, 19.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 334/577 [00:16<00:12, 19.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▊    | 338/577 [00:16<00:11, 20.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 342/577 [00:17<00:11, 20.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████▉    | 345/577 [00:17<00:11, 20.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 348/577 [00:17<00:11, 20.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 351/577 [00:17<00:11, 20.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 355/577 [00:17<00:10, 20.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 358/577 [00:17<00:10, 20.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 361/577 [00:17<00:10, 20.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 364/577 [00:17<00:10, 20.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▎   | 367/577 [00:18<00:10, 20.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▍   | 370/577 [00:18<00:10, 20.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▍   | 373/577 [00:18<00:10, 20.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 376/577 [00:18<00:09, 20.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 379/577 [00:18<00:09, 20.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 382/577 [00:18<00:09, 20.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 385/577 [00:19<00:09, 20.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 390/577 [00:19<00:09, 20.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 393/577 [00:19<00:09, 20.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▊   | 396/577 [00:19<00:08, 20.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 399/577 [00:19<00:08, 20.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████▉   | 402/577 [00:19<00:08, 20.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 405/577 [00:19<00:08, 20.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 408/577 [00:19<00:08, 20.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 411/577 [00:20<00:08, 20.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 414/577 [00:20<00:08, 20.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 416/577 [00:20<00:07, 20.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 419/577 [00:20<00:07, 20.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 423/577 [00:20<00:07, 20.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 426/577 [00:20<00:07, 20.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 430/577 [00:20<00:07, 20.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 433/577 [00:21<00:07, 20.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 436/577 [00:21<00:06, 20.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 439/577 [00:21<00:06, 20.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 442/577 [00:21<00:06, 20.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 445/577 [00:21<00:06, 20.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 449/577 [00:21<00:06, 20.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 452/577 [00:22<00:06, 20.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 456/577 [00:22<00:05, 20.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████▉  | 459/577 [00:22<00:05, 20.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 462/577 [00:22<00:05, 20.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 465/577 [00:22<00:05, 20.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 472/577 [00:22<00:05, 20.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 475/577 [00:22<00:04, 20.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 479/577 [00:23<00:04, 20.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▎ | 483/577 [00:23<00:04, 20.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 487/577 [00:23<00:04, 20.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 491/577 [00:23<00:04, 20.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 494/577 [00:23<00:03, 20.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 497/577 [00:23<00:03, 20.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 501/577 [00:24<00:03, 20.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 504/577 [00:24<00:03, 20.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 507/577 [00:24<00:03, 20.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 510/577 [00:24<00:03, 20.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 513/577 [00:24<00:03, 20.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 516/577 [00:24<00:02, 20.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 520/577 [00:24<00:02, 20.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 525/577 [00:25<00:02, 20.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 532/577 [00:25<00:02, 21.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 536/577 [00:25<00:01, 21.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▎| 540/577 [00:25<00:01, 21.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 544/577 [00:25<00:01, 21.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▍| 548/577 [00:25<00:01, 21.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 552/577 [00:25<00:01, 21.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 557/577 [00:26<00:00, 21.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 561/577 [00:26<00:00, 21.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 565/577 [00:26<00:00, 21.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▊| 569/577 [00:26<00:00, 21.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 573/577 [00:26<00:00, 21.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████▉| 576/577 [00:26<00:00, 21.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 577/577 [00:26<00:00, 21.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "myML=[]\n",
    "myKeyL=[]\n",
    "for i_th,(key,sent) in enumerate(tqdm(inTestStrS.items(),total=len(inTestStrS))):\n",
    "    sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)## remove non alpha numeric \n",
    "    tokens=re.split(pattern=' ',string=sent)\n",
    "    #\n",
    "    tokens=list(filter(lambda token:(token!='') and (token not in stopWords)  ,tokens))\n",
    "    sent=' '.join(tokens)\n",
    "    ###keep track of each token\n",
    "    scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "    #for n_gram in range(1,len(tokens)+1):\n",
    "    myNMax=min( [len(tokens),phraseMax])\n",
    "    #print (myNMax)\n",
    "    for n_gram in range(1,myNMax+1):\n",
    "        grams=list(map(lambda L:\" \".join(L),list(ngrams(tokens,n_gram))))\n",
    "        val_docs = list(nlp.pipe(grams))\n",
    "        val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "        predictM=model.predict_proba(val_X)\n",
    "        tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "        empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "        moreThanTwoValToken_mask=(val_X!=0).sum(axis=1)>=2\n",
    "        tmpDf[empty_mask&moreThanTwoValToken_mask]=0\n",
    "        for i,gram in enumerate(tmpDf.index): #i: track the each token position\n",
    "            #for j,one_gram in enumerate(gram.split(' ')):\n",
    "            i_end=i+n_gram+1\n",
    "            textBefore=\" \".join(tokens[:i]) + ('' if i==0 else ' ')\n",
    "            start_char_pos=len(textBefore)\n",
    "            myKeyL.append(key+(i_th,sent,n_gram,i,i_end,gram,start_char_pos)) \n",
    "            myML.append(tmpDf.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textBefore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp=myKeyL[2]\n",
    "#tmp[1][6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpDf.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#tmpDf.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thresholdS=pd.Series(\\n   {'SCIENTIFIC_NAME': 0.26689231395721436,\\n 'cell type': 0.10027739964425564,\\n 'disease': 0.15823280811309814,\\n 'genotype': 0.09954400360584259,\\n 'geo_loc_name': 0.6163255572319031,\\n 'sex': 0.6458048224449158}\\n)\""
      ]
     },
     "execution_count": 1506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"thresholdS=pd.Series(\n",
    "   {'SCIENTIFIC_NAME': 0.26689231395721436,\n",
    " 'cell type': 0.10027739964425564,\n",
    " 'disease': 0.15823280811309814,\n",
    " 'genotype': 0.09954400360584259,\n",
    " 'geo_loc_name': 0.6163255572319031,\n",
    " 'sex': 0.6458048224449158}\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf=pd.concat(myML,keys=myKeyL,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf.index.names=['srs','attribute','n_sent']+['i_thSrs','orig_text','n','i','i_end','token','ith_char_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf_sub=mergedDf[mergedDf.index.get_level_values('n')>=n_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1527,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(scoreMargin_m),len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxS=mergedDf_sub.max(axis=1)#>0.3\n",
    "secondBestScoreS=mergedDf_sub.quantile(0.999,interpolation='lower',axis=1)\n",
    "scoreMargin_m=(maxS-secondBestScoreS)>0.1\n",
    "m_val=scoreMargin_m&(~mergedDf_sub.index.get_level_values('token').str.contains('[0-9 ]+ [0-9 ]+'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=pd.DataFrame({'predicted':mergedDf_sub[m_val].idxmax(axis=1),'score':mergedDf_sub[m_val].max(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "scoreSortedDf=tmpDf[m_val].sort_values(['orig_text','i','score'],ascending=False).reset_index(\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all overlapping regions\n",
    "v=scoreSortedDf.copy()\n",
    "scoreSortedDf=scoreSortedDf.assign(OverlapGroup=(len(inTestStrS)*v.i_thSrs+ \n",
    "                                          (v.i_end - v.i.shift(-1)).shift().lt(0).cumsum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srs</th>\n",
       "      <th>attribute</th>\n",
       "      <th>n_sent</th>\n",
       "      <th>i_thSrs</th>\n",
       "      <th>orig_text</th>\n",
       "      <th>n</th>\n",
       "      <th>i</th>\n",
       "      <th>i_end</th>\n",
       "      <th>token</th>\n",
       "      <th>ith_char_pos</th>\n",
       "      <th>predicted</th>\n",
       "      <th>score</th>\n",
       "      <th>OverlapGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SRS345653</td>\n",
       "      <td>description</td>\n",
       "      <td>3</td>\n",
       "      <td>280</td>\n",
       "      <td>vancomycin 500 mg l</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>mg l</td>\n",
       "      <td>15</td>\n",
       "      <td>treatment</td>\n",
       "      <td>0.171350</td>\n",
       "      <td>161560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRS345653</td>\n",
       "      <td>description</td>\n",
       "      <td>3</td>\n",
       "      <td>280</td>\n",
       "      <td>vancomycin 500 mg l</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>500 mg</td>\n",
       "      <td>11</td>\n",
       "      <td>treatment</td>\n",
       "      <td>0.211407</td>\n",
       "      <td>161560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ERS710488</td>\n",
       "      <td>description</td>\n",
       "      <td>5</td>\n",
       "      <td>416</td>\n",
       "      <td>using low input protocol</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>input protocol</td>\n",
       "      <td>10</td>\n",
       "      <td>treatment</td>\n",
       "      <td>0.137796</td>\n",
       "      <td>240032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ERS710488</td>\n",
       "      <td>description</td>\n",
       "      <td>5</td>\n",
       "      <td>416</td>\n",
       "      <td>using low input protocol</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>low input</td>\n",
       "      <td>6</td>\n",
       "      <td>genotype</td>\n",
       "      <td>0.394349</td>\n",
       "      <td>240032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRS019017</td>\n",
       "      <td>description</td>\n",
       "      <td>2</td>\n",
       "      <td>188</td>\n",
       "      <td>using Dynabeads mRNA DIRECT Micro Kit Invitrogen</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Micro Kit</td>\n",
       "      <td>28</td>\n",
       "      <td>geo_loc_name</td>\n",
       "      <td>0.527912</td>\n",
       "      <td>108476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         srs    attribute  n_sent  i_thSrs  \\\n",
       "0  SRS345653  description       3      280   \n",
       "1  SRS345653  description       3      280   \n",
       "2  ERS710488  description       5      416   \n",
       "3  ERS710488  description       5      416   \n",
       "4  DRS019017  description       2      188   \n",
       "\n",
       "                                          orig_text  n  i  i_end  \\\n",
       "0                               vancomycin 500 mg l  2  2      5   \n",
       "1                               vancomycin 500 mg l  2  1      4   \n",
       "2                          using low input protocol  2  2      5   \n",
       "3                          using low input protocol  2  1      4   \n",
       "4  using Dynabeads mRNA DIRECT Micro Kit Invitrogen  2  4      7   \n",
       "\n",
       "            token  ith_char_pos     predicted     score  OverlapGroup  \n",
       "0            mg l            15     treatment  0.171350        161560  \n",
       "1          500 mg            11     treatment  0.211407        161560  \n",
       "2  input protocol            10     treatment  0.137796        240032  \n",
       "3       low input             6      genotype  0.394349        240032  \n",
       "4       Micro Kit            28  geo_loc_name  0.527912        108476  "
      ]
     },
     "execution_count": 1532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreSortedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf=scoreSortedDf.sort_values(['OverlapGroup','score'],ascending=False).drop_duplicates(['OverlapGroup','predicted']\n",
    "                                                                                   ).sort_values('orig_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['token_len']=hitDf['token'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['recovered_txt']=hitDf.apply(\n",
    "    lambda tmpS2:tmpS2.loc['orig_text'][tmpS2.loc['ith_char_pos']:(tmpS2.loc['ith_char_pos']+tmpS2.loc['token_len'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is it extraction:  True\n"
     ]
    }
   ],
   "source": [
    "print ('is it extraction: ',(hitDf.recovered_txt==hitDf.token).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}\n",
    "\\\n",
    "Content contains input text, annotation has the labeled content, extras is for some extra columns that you want to show with each row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClassToId={}\n",
    "for myClass in le.classes_:\n",
    "    myClassToId[myClass]=nlp.vocab.strings.add(myClass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [],
   "source": [
    "classToSpacyId=pd.Series(myClassToId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDisplayHitDf=hitDf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDisplayHitDf['predicted_entity_id']=classToSpacyId.loc[inDisplayHitDf['predicted']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inDisplayHitDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inTestStrFlatS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text_S=inDisplayHitDf.drop_duplicates(['srs','n_sent','orig_text']).set_index(['srs','n_sent'])['orig_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_textL=orig_text_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABLCAYAAABz9YPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAArBJREFUeJzt3aFqlmEYxvH7FedWFkQHDkQZw+aCuKqIJyFosO0UPASbWCxiswkegh6BNkUwDJsMJsKSYfJYLBY/xnh49r3X79cGT7jutD+8g02ttQIASHVu9AAAgJHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHOL3owTdNeVe1VVU0ra7dXLl3tPmqUnWl/9ISuPq9eGD2hm+3voxf0dbR+bfSErtrvg9ETurq4emX0hK4Op6PRE7pZX/8xekJX+7U9ekJXx1+/HLbWNha9m07y7zhWN2+0zcfPTzXsLPu29nD0hK52tub7C/XN0+PRE7p6f+/F6Ald/fr5bPSErh5sPRk9oatXa+9GT+jmzt3Xoyd09Wh6O3pCVwf3b31sre0ueuczGQAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANGm1tr/H0zTXlXt/f3xZlV96j1qoMtVdTh6RCdzvq3KfcvOfctrzrdVuW/ZXW+tbSx6tDCG/nk8TR9aa7unmnWGzfm+Od9W5b5l577lNefbqtyXwmcyACCaGAIAop00hl52WXF2zPm+Od9W5b5l577lNefbqtwX4UR/MwQAMDc+kwEA0cQQABBNDAEA0cQQABBNDAEA0f4Az7tyiUpEylQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)\n",
    "colors = {ent.upper():matplotlib.colors.to_hex(myColor)  for myColor,ent in zip(current_palette,classToSpacyId.index)}\n",
    "options = {'ents': classToSpacyId.index.str.upper(),\n",
    "           'colors': colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf_groupby=inDisplayHitDf.groupby('srs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srs         n_sent\n",
       "ERS098274   4         000 cm2 laminin coated vessels 10 ug ml lamini...\n",
       "            1         1 1 mixture DMEM F 12 Neurobasal media Invitro...\n",
       "SRS703382   4                                              1 DMSO 4 hrs\n",
       "SRS150400   11                           1 mM non essential amino acids\n",
       "            9                                      1 mM sodium pyruvate\n",
       "SRS010066   2                                          10 ug mL insulin\n",
       "            3                10 ug mL transferrin differentiation media\n",
       "SRS988108   7                                          1038 nature14582\n",
       "ERS710418   6                                                 1073 pnas\n",
       "ERS042908   4                                           10ug mL Insulin\n",
       "ERS508289   5         1ug total RNA used make poly A selected barcod...\n",
       "SRS150400   8                                          2 mM L glutamine\n",
       "ERS1269995  3                              2 mM L glutamine G7513 100ML\n",
       "ERS215384   2         2001 Fractionation Ter119 Ter119 cells Milteny...\n",
       "            3         2005 Formaldehyde crosslinked chromatin 10 7 T...\n",
       "ERS025101   4         2010 using 8 10 x 10 7 U2OS FOXK2 HF cells sta...\n",
       "ERS042908   5                        20ng mL EGF 100ng mL cholera toxin\n",
       "SRS214866   1                       4 children formula fed 1 breast fed\n",
       "SRS373257   1         4 control 5 AD cases brain bank Emory Universi...\n",
       "ERS580098   4         4 ug total RNA used library preparation TruSeq...\n",
       "SRS000090   6         40 bp deletion occurred beginning exon 5 bp 64...\n",
       "ERS494760   2                      48h post seeding splitting new plate\n",
       "ERS215384   1         5 C57 BL6 mouse embryos expanded 3 days serum ...\n",
       "SRS703382   1                     5 FBS endothelial basal medium 12 hrs\n",
       "ERS042908   1                                             5 Horse serum\n",
       "ERS710488   1         5 adult Swiss Webster mice grown non adherent ...\n",
       "ERS610099   7                                        5 average coverage\n",
       "ERS1078730  1                                   5 library prep protocol\n",
       "SRS703382   3                                                5 medium 0\n",
       "SRS150400   12               5 mg ml penicillin streptomycin Invitrogen\n",
       "                                            ...                        \n",
       "ERS098274   5         passaged confluence using Accutase dissociatio...\n",
       "            7                              phenol chloroform extraction\n",
       "SRS1274330  0           pilot study shRNA pool transfected mouse testis\n",
       "SRS254654   2                  poly A RNA purified using oligo dT beads\n",
       "ERS1269995  13                               poly A selection total RNA\n",
       "ERS717101   2         presence IL4 10ng ml Libraries prepared accord...\n",
       "ERS017587   1                  presence vaginal plug noon considered E0\n",
       "SRS1145978  2             removed cardia stomach region 66 year old man\n",
       "SRS000090   7                    resulting deletion amino acids 215 227\n",
       "            3                                       resulting truncated\n",
       "SRS004335   1                 resuspended 250 uL PBS isolated Trizol LS\n",
       "ERS346145   3         second time different flowcell 51 bp single en...\n",
       "ERS1672820  1            single cell suspensions generated lysed QIAzol\n",
       "SRS254654   5                                size selected agarose gels\n",
       "ERS1544402  2                                     snoRD50a ASOs treated\n",
       "DRS012555   0         species Mus musculus strain strain A albino mo...\n",
       "DRS012579   0         species Mus musculus strain strain A albino mo...\n",
       "SRS348125   9                split 5 technical replicates sequencing R1\n",
       "SRS000594   1         subsequent submission sample establishment cel...\n",
       "ERS661069   2         subsequently prmt1 kdm4c expression inhibited ...\n",
       "SRS988108   0         tdTomato cardiomyocytes Mus musculus CAG creER...\n",
       "SRS348125   3                                          tm1Frk tm1Frk gt\n",
       "ERS872196   2         total RNAs purified Trizol GT miRNeasy kit wic...\n",
       "SRS254654   1            treated RNase free DNase I remove residual DNA\n",
       "SRS418305   5                                   unique high probability\n",
       "ERS1544402  4                            used RNAi max ASO transfection\n",
       "ERS1129240  2         using Bravo Automated Liquid Handling Platform...\n",
       "DRS019017   2          using Dynabeads mRNA DIRECT Micro Kit Invitrogen\n",
       "ERS710488   5                                  using low input protocol\n",
       "SRS345653   3                                       vancomycin 500 mg l\n",
       "Name: orig_text, Length: 375, dtype: object"
      ]
     },
     "execution_count": 1547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_textL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_L=[]\n",
    "hitDf_groupby=inDisplayHitDf.groupby('orig_text')\n",
    "for key,orig_text in orig_textL.items():#[10]\n",
    "    \n",
    "\n",
    "    inRecordDisplayDf=hitDf_groupby.get_group(orig_text)\n",
    "\n",
    "    doc=nlp(orig_text)\n",
    "\n",
    "    for _,tmpS in inRecordDisplayDf.iterrows():\n",
    "        EVENT=tmpS['predicted_entity_id']\n",
    "        entity=(EVENT,tmpS['i'],tmpS['i_end']-1)# this the correct one\n",
    "        #print (entity)\n",
    "        doc.ents+=(entity,)\n",
    "        \n",
    "        #optional\n",
    "        title=\"{}: sentence #{}  \".format(key[0],key[1])\n",
    "        doc.user_data['title']=title\n",
    "    doc_L.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_textL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inDisplayHitDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.displacy.render(doc, style='ent',jupyter=True,options=options)\n",
    "\n",
    "html=spacy.displacy.render(doc_L, style='ent',page=True,options=options)\n",
    "\n",
    "with open('./Data/displacy.{}.html'.format(selectedValidAttrib),'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!echo $PWD/./Data/displacy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1491,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfasf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1491-9f9b4d966f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdfasf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfasf' is not defined"
     ]
    }
   ],
   "source": [
    "asdfasf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerName='cell type'\n",
    "nlp.vocab.strings.add(nerName)\n",
    "EVENT = nlp.vocab.strings[nerName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188603156775490070"
      ]
     },
     "execution_count": 1464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1390,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"t cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use \n",
    "entity=(EVENT,0,len(u\"t cell\"))\n",
    "doc.ents+=(entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    t cell\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">cell type</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style='ent',jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc\n",
    "    doc.ents += (entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myToke' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1394-d2a78d08352e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyToke\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myToke' is not defined"
     ]
    }
   ],
   "source": [
    "myToke.label=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1395-66d093152305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "doc.ents+=tuple(['cell type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span=doc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span.label='cell type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export to dataturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myF(tmpS1):\n",
    "    return {\"label\":[tmpS1.loc['predicted']],\"points\":{'start':tmpS1.loc['i'],\n",
    "     'end':tmpS1.loc['i_end']-1,'text':tmpS1.loc['token']},\n",
    "\n",
    "        }\n",
    "\n",
    "inAnnotDf=hitDf.copy()\n",
    "myL=[]\n",
    "for text,subDf in inAnnotDf.groupby(['orig_text']):\n",
    "    oneAnnotatedLine={\"content\":text,'annotation':list(subDf.apply(myF,axis=1)),\n",
    "                                \"extras\":None,\n",
    "        \"metadata\":{\"first_done_at\":1535058971000,\n",
    "                                     \"last_updated_at\":1535058971000,\"sec_taken\":0,\n",
    "                                \"last_updated_by\":\"EEOBDlEO48T6gzo0KHvT0IkqZnn2\"}}\n",
    "    myL+=[json.dumps(oneAnnotatedLine)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( './Data/validation.ngram.json.txt','w')as f:\n",
    "    f.write(\"\\n\".join(myL[:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Data/validation.ngram.json.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\",\".join(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneAnnotatedLine['annotation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDict={\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}\n",
    "exampleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}\n",
    "#myL=[]\n",
    "#{\"content\":text,\"annotation\":}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inAnnotDf.iloc[:1].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['i_thSrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['i_thSrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf.sort_values(['i_thSrs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNameL=list(VC.index.names)\n",
    "myNameL[-1]='Predicted_NE'\n",
    "VC.index.names=myNameL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for each n-gram, there is the same start site, use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.drop_duplicates(['orig_text','Predicted_NE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDf\n",
    "#take on any length, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=(mergedDf>thresholdS).loc['ESCs WT replicate1 mRNA Mad2l2'].stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf.columns=['n','n-gram','attrib','passThreshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpDf['n-gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf[tmpDf.passThreshold]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpS[tmpS].groupby(level=[1,2]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf2.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf=pd.concat(myML,keys=list(inTestStrS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_token_m=mergedDf.index.get_level_values(0).str.contains('^\\d+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf=mergedDf[~numeric_token_m].copy()#.loc[:,mergedDf.columns!='age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.800000\n",
    "mergedSubDf['undetected']=threshold\n",
    "#noAmbigM=(mergedSubDf>=threshold).sum(axis=1)>=1 #this mark screw up the confoudning boundary \n",
    "mergedSubDf.loc[:,'undetected']=threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf=mergedSubDf.idxmax(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf.index.names=['Freetext','Token']\n",
    "predDf['token_numeric']=predDf.index.get_level_values('Token').str.contains('^\\d+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel=pd.ExcelWriter('./Results/for_curation.xlsx')\n",
    "predDf[~predDf['token_numeric']].to_excel(excel)\n",
    "excel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf[~predDf['token_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Results/for_curation.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Results/for_curation.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf[~predDf['token_numeric']].to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm tmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with duplicated states\n",
    "#emptyStat=np.array([0.42332533, 0.4360587 , 0.61020947, 0.42082471, 0.4110575 ,\n",
    "#       0.42533568, 0.47932082])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyState=mergedSubDf.groupby(mergedSubDf.columns.tolist(),as_index=False).size().sort_values().index[-1]\n",
    "#emptyStat=np.array([0.42332533, 0.4360587 , 0.61020947, 0.42082471, 0.4110575 ,\n",
    "#       0.42533568, 0.47932082])\n",
    "print (emptyState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noPredM=((mergedSubDf-emptyState).abs()<0.1).all(axis=1)\n",
    "mergedSubDf[(~noPredM)&(mergedSubDf>0.5).sum(axis=1)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good examples: HAP1 LMTK3-KO cells, stimulated with WNT3, replicate R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf.loc['HAP1 CCK4-KO cells, stimulated with RESV, replicate R1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf[].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=scoreDf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##among the ones with >0.5, take the ones that are unique\n",
    "sns.heatmap(data=(scoreDf>0.6).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=scoreDf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### among the ones with clear boundry, it can classify well. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "argue that it can salvage the data correctly. Among those sentences, \n",
    "\n",
    "take >0.5 as boundary, run top 10000 sentences \n",
    "\"\"\"\n",
    "scoreDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent='Human histone H3 di-methylated at lysine 4 (H3K4me2) in human blood CD4+ T cells, targeted using Abcam antibody ab7766'#inTestStrS.iloc[5]\n",
    "sent='RNA-seq of total RNA from Z/Edn2; Six3-Cre mouse retina\t'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "#print (sent)\n",
    "tokens=re.split(pattern=' ',string=sent)\n",
    "s=tokens\n",
    "#print (tokens)\n",
    "scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "#for n_gram in range(1,len(tokens)+1):\n",
    "#for n_gram in range(1,len(tokens)):\n",
    "n_gram=8\n",
    "grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "#print (grams)\n",
    "val_docs = list(nlp.pipe(grams))\n",
    "val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "tmpDf=pd.DataFrame(data=model.predict_proba(val_X),columns=le.classes_,index=grams)\n",
    "#tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "tmpDf[empty_mask]=0\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(3,2.5*4))\n",
    "sns.heatmap(tmpDf,annot=True,ax=ax,vmin=0,vmax=1.0,fmt='.2f',cbar=None)\n",
    "#ax.set_xticklabels([])\n",
    "\"\"\"break\n",
    "\n",
    "#each n gram only advange \n",
    "for i,gram in enumerate(tmpDf.index):# for ec\n",
    "    for j,one_gram in enumerate(gram.split(' ')):\n",
    "        scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each one, makes a prediction on the term, to see what it is supposed to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "#print (sent)\n",
    "tokens=re.split(pattern=' ',string=sent)\n",
    "s=tokens\n",
    "#print (tokens)\n",
    "scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "#for n_gram in range(1,len(tokens)+1):\n",
    "for n_gram in range(1,len(tokens)):\n",
    "    grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "    #print (grams)\n",
    "    val_docs = list(nlp.pipe(grams))\n",
    "    val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "    predictM=model.predict_proba(val_X)\n",
    "\n",
    "    tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "    empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "    tmpDf[empty_mask]=0\n",
    "\n",
    "    \"\"\"\n",
    "    each n gram only advange \n",
    "    \"\"\"\n",
    "    for i,gram in enumerate(tmpDf.index):# for ec\n",
    "        for j,one_gram in enumerate(gram.split(' ')):\n",
    "            scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.2\n",
    "scoreDf[scoreDf<=threshold]=0\n",
    "scoreDf['undetected']=threshold\n",
    "\n",
    "scoreDf.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scoreDf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scoreDf.T,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexS=srsS[srsS.index.get_level_values(1)=='sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSubSrsS1=srsS[srsS.str.contains('rna',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSubSrsS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
