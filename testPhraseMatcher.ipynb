{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create entity \n",
    "\n",
    "How to do the maximal match\n",
    "\n",
    "1. tokenization phrase,  \n",
    "2. embedding\n",
    "3. do a classification on the text data\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_table=pd.read_pickle('./Data/NCIT_table.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the semantic type as entity types\n",
    "\n",
    ", synonyms , the pipeline accept theurapeutic or preventive procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_df['semantic type'].value_counts().to_csv('semantic_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantics_ignore=['Qualitative Concept']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df[NCIT_df['semantic type'].isin(semantics_ignore)].sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df\n",
    "\n",
    "NCIT_df=NCIT_table.reset_index().drop_duplicates(['code', 'concept name', 'parents', 'definition', 'semantic type',\n",
    "       'synonyms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare patterns\n",
    "\n",
    "input: synonyms\n",
    "\n",
    "\n",
    "\n",
    "output: tokenized synonyns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x2b8f5bb27a58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCIT_df.groupby('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of synonym: 388961\n",
      "# of multi mapped: 13603\n"
     ]
    }
   ],
   "source": [
    "synonymVC=NCIT_df['synonyms'].value_counts()\n",
    "print ('# of synonym:',len(synonymVC))\n",
    "print ('# of multi mapped:',(synonymVC>1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_df['synonym_word_len']=NCIT_df['synonyms'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_df['synonyms_lower']=NCIT_df['synonyms'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=NCIT_df.synonyms.isin(synonymVC.index[synonymVC==1])\n",
    "\n",
    "m_test=NCIT_df.synonyms_lower.isin(['tp53','t cell','cell'])\n",
    "\n",
    "m_text_length=NCIT_df.synonym_word_len>=3\n",
    "\n",
    "\"\"\"\n",
    "ascending=True: t, Cell \n",
    "\"\"\"\n",
    "ncit_sub_df=NCIT_df[m_text_length].drop_duplicates(\n",
    "    ['code','synonyms_lower']\n",
    "     ).sort_values('n_th synonym').drop_duplicates('synonyms_lower')#.sort_values('synonym_word_len',ascending=True)#.sort_values('n_th synonym',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-20b183515a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnerName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCI'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnerName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/miniconda3/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/miniconda3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "nerName='NCI'\n",
    "nlp.vocab.strings.add(nerName)\n",
    "EVENT = nlp.vocab.strings[nerName]\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc\n",
    "    doc.ents += (entity,)\n",
    "\"\"\"\n",
    "for each synonyms, make a pattern \n",
    "\"\"\"\n",
    "for _,tmpDf in  tqdm(ncit_sub_df.groupby('code')):\n",
    "    \n",
    "    #myEntityType=tmpDf['semantic type'].iloc[0]\n",
    "    term_name=tmpDf['synonyms'].iloc[0]\n",
    "    try:\n",
    "        myPatterns=[]\n",
    "        for  mySynonym in tmpDf['synonyms_lower'].tolist():\n",
    "            mySynonymSplitted=mySynonym.split(' ')\n",
    "            mySynonymSplitted=list(filter(lambda s: len(s)>0,mySynonymSplitted))\n",
    "            ##add tokens if it doens't exist\n",
    "            for myToken in mySynonymSplitted :\n",
    "                if myToken not in  nlp.vocab.strings:\n",
    "                    nlp.vocab.strings.add(myToken)              \n",
    "            myPatterns.append([{'LEMMA':myToken} for myToken in mySynonymSplitted])\n",
    "        args=[term_name,add_event_ent]+myPatterns\n",
    "        matcher.add(*args)\n",
    "        \n",
    "    except ValueError:\n",
    "        print ('failed ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize the srs data\n",
    "\n",
    "\n",
    "input: list of sentences, \n",
    "output: list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_pickle\n",
    "all_inS=pd.read_pickle(inS_dir)\n",
    "#srxS=pd.read_pickle('/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRX.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  test \n",
    "inS=all_inS.sample(n=10,random_state=2)#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=nlp.pipe(inS.tolist(),n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "2it [00:00, 13.55it/s]\u001b[A\n",
      "4it [00:00, 14.12it/s]\u001b[A\n",
      "8it [00:00, 20.74it/s]\u001b[A\n",
      "10it [00:00, 24.73it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "matches_L=[]\n",
    "doc_L=[]\n",
    "for doc in tqdm(docs):\n",
    "    matches=matcher(doc)\n",
    "    matches_L.append(matches)\n",
    "    doc_L.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>concept name</th>\n",
       "      <th>parents</th>\n",
       "      <th>definition</th>\n",
       "      <th>semantic type</th>\n",
       "      <th>n_th synonym</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>synonym_word_len</th>\n",
       "      <th>synonyms_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270703</th>\n",
       "      <td>C51179</td>\n",
       "      <td>&lt;http://ncicb.nci.nih.gov/xml/owl/EVS/Thesauru...</td>\n",
       "      <td>C21381</td>\n",
       "      <td>Human GRK2 wild-type allele is located in the ...</td>\n",
       "      <td>Gene or Genome</td>\n",
       "      <td>3</td>\n",
       "      <td>BARK</td>\n",
       "      <td>4</td>\n",
       "      <td>bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363783</th>\n",
       "      <td>C79615</td>\n",
       "      <td>&lt;http://ncicb.nci.nih.gov/xml/owl/EVS/Thesauru...</td>\n",
       "      <td>C79611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plant</td>\n",
       "      <td>0</td>\n",
       "      <td>Bark</td>\n",
       "      <td>4</td>\n",
       "      <td>bark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          code                                       concept name parents  \\\n",
       "270703  C51179  <http://ncicb.nci.nih.gov/xml/owl/EVS/Thesauru...  C21381   \n",
       "363783  C79615  <http://ncicb.nci.nih.gov/xml/owl/EVS/Thesauru...  C79611   \n",
       "\n",
       "                                               definition   semantic type  \\\n",
       "270703  Human GRK2 wild-type allele is located in the ...  Gene or Genome   \n",
       "363783                                                NaN           Plant   \n",
       "\n",
       "        n_th synonym synonyms  synonym_word_len synonyms_lower  \n",
       "270703             3     BARK                 4           bark  \n",
       "363783             0     Bark                 4           bark  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCIT_df[NCIT_df.synonyms_lower.isin(['bark'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12265437080716940157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9625438048578609369</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0  1  2\n",
       "0  12265437080716940157  0  1\n",
       "1   9625438048578609369  2  3"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Human\n",
       "1     Bark\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpDf=pd.DataFrame(matches_L[1])\n",
    "tmpDf[0].apply(lambda s:nlp.vocab.strings[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings[9625438048578609369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Human frontal cortex]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpDoc=doc_L[1]\n",
    "list(tmpDoc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a word2vect \n",
    "\n",
    "for the data, tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "specieS=sampleS[sampleS.index.get_level_values(1)=='SCIENTIFIC_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedSrs=specieS[specieS=='Mus musculus'].index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampleS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 256 ms, total: 2.52 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mouseSrsS=sampleS[sampleS.index.get_level_values(0).isin(selectedSrs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampleS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouseAttribVC=mouseSrsS.index.get_level_values(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#mouseSrsS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=mouseSrsS.groupby(level=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERS949109  SCIENTIFIC_NAME              Mus musculus\n",
       "           SUBMITTER_ID          IFMX1_1D-sc-2454762\n",
       "           strain                                   \n",
       "           sample_description    Lin_neg_bone marrow\n",
       "           ArrayExpress-Sex                   female\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouseSrsS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA          DNA\n",
      "Isolation          Isolation\n",
      "Kit          Kit\n",
      "Brain          Brain\n",
      "not          Negation\n",
      "determined          Determine\n",
      "hypothalamus          Hypothalamus\n",
      "whole          Whole\n",
      "genome          Genome\n",
      "sequence          Sequence\n",
      "strains          Strain\n",
      "Mus          Mouse\n",
      "Mus musculus          Mus musculus\n"
     ]
    }
   ],
   "source": [
    "inL=mouseSrsS.sample(n=1000).tolist()\n",
    "for i,send in enumerate(inL):\n",
    "    doc=nlp(send)\n",
    "    matches=matcher(doc)\n",
    "    for match in matches:\n",
    "        print (doc[match[1]:match[2]],'        ',nlp.vocab.strings[match[0]])\n",
    "    if i>10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df[NCIT_df.synonyms.isin(['Mus musculus','Mouse'])]['definition'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Screen 12 Lmnb2 Knock-out Embryonic Stem Cell Derived Macrophages stimulated with Salmonella"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Streptococcus pneumoniae\n",
       "Streptococcus pneumoniae\n",
       "SN20648-sc-2011-11-03T19:59:41Z-1101437\n",
       "\n",
       "\n",
       "not applicable\n",
       "Streptococcus pneumoniae\n",
       "Pathogen sample from Salmonella enterica\n",
       "Salmonella enterica subsp."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['percutaneous',\n",
       " 'coronary',\n",
       " 'intervention',\n",
       " '(pci)',\n",
       " 'for',\n",
       " 'high',\n",
       " 'risk',\n",
       " 'non-st',\n",
       " 'elevation',\n",
       " 'myocardial',\n",
       " 'infarction',\n",
       " 'or',\n",
       " 'unstable',\n",
       " 'angina']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySynonymSplitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'glioblastoma cell line TP53 t cell')\n",
    "matches = matcher(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "##word t probably map \n",
    "#lexem=doc.vocab['t']\n",
    "#lexem.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glioblastoma Glioblastoma\n",
      "cell Cell\n",
      "cell line Cell Line\n",
      "line Long Interspersed Element\n",
      "line Line\n",
      "TP53 TP53 Gene\n",
      "t cell T-Lymphocyte\n",
      "cell Cell\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print (doc[match[1]:match[2]],nlp.vocab.strings[match[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10878580257448234319, 5, 6)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP53 NCI\n",
      "t NCI\n",
      "cell NCI\n"
     ]
    }
   ],
   "source": [
    "for a in doc.ents:\n",
    "    print (a,a.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP53 t cell"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EVENT = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP53 <function add_event_ent at 0x2b99bc527c80> [{'LEMMA': 'tp53'}]\n"
     ]
    }
   ],
   "source": [
    "def f(a,b,c):\n",
    "    print (a,b,c)\n",
    "f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'LEMMA': 'tp53'}]]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gene or Genome'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myEntityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8984921151469646652"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tp53, t cell)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFirst=NCIT_df.groupby('code')['synonyms'].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myVC=myFirst.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(myVC[myVC>1].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrthsToPatterns(L):\n",
    "    L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSynonymS.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleS=pd.read_pickle('/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher=Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp.make_doc(u'London is a big city in the United Kingdom.')\n",
    "print('Before', list(doc.ents))  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)))\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[u'GPE']\n",
    "doc.from_array(header, attr_array)\n",
    "print('After', list(doc.ents))  # [London]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge('NNP' if label else span.root.tag_, span.text, nlp.vocab.strings[label])\n",
    "    \n",
    "    matcher.add()\n",
    "matcher.add('GoogleNow', merge_phrases, [{ORTH: 'Google'}, {ORTH: 'Now'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_match(matcher, doc, id, matches):\n",
    "    print('Matched!', matches)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HelloWorld', on_match, [{'LOWER': 'hello'}, {'LOWER': 'world'}])\n",
    "matcher.add('GoogleMaps', on_match, [{'ORTH': 'Google'}, {'ORTH': 'Maps'}])\n",
    "doc = nlp(u'HELLO WORLD on Google Maps.')\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=doc.ents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/linguistic-features#on_match\n",
    "\n",
    "Adding on_match rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3492015959717556363, 5, 9), (3492015959717556363, 5, 10)]\n",
      "[(3492015959717556363, 5, 9), (3492015959717556363, 5, 10)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "##entity label, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent=doc.ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "using a function to define different entity in princ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "EVENT = nlp.vocab.strings['cell']\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    #print (matches)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc.ents += (entity,)\n",
    "\n",
    "    \n",
    "matcher.add('cell', add_event_ent,\n",
    "            [{'ORTH': 'cell'}])\n",
    "EVENT = nlp.vocab.strings['google']\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    #print (matches)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc.ents += (entity,)\n",
    "matcher.add('GoogleIO', add_event_ent,\n",
    "            [{'ORTH': 'google'}])\n",
    "           # [{'ORTH': 'Google'}, {'ORTH': 'I'}, {'ORTH': '/'}, {'ORTH': 'O'}, {'IS_DIGIT': True}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"cell is google. 10 days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.vocab.strings['Therapeutic or Preventive Procedure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ent=doc.ents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings['cell']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
