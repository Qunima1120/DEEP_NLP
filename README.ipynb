{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP learning in bio NLP\n",
    "\n",
    "### background\n",
    "Natural Languague Processing (NLP) used to be one of the most feature engineering intense fields like Computer Vision. \n",
    "Deep learning so far has been hugely successful in computer vision undoubtly. \n",
    "From my undergrad to phd, I have personally been mind blown by deep learning has transformed the paradigm of vision field. It used to take years of painful feature engineering to gain modest improvements. Now it take only day(s) to write <100 lines of code to yield an accuracy that finds real world applications in days. \n",
    "\n",
    "NLP is somewhat undergoing the same transformation. The particular field in consideration in here is biological NLP (bioNLP).\n",
    "Biological text is complicated and most of the current bio NLP approaches rely on manual feature engineering. For example, in tokenization, synonyms of gene names idealistically can map to the same token, but gene names are inherently getting updated for human over time. \n",
    "On of the most successful story in NLP is Google. \n",
    "\n",
    "The key characteristic of why Google beat the competition was that the search was curation free.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  notebooks\n",
    "\n",
    "|Code| Usage| \n",
    "|:--------------:|------:|\n",
    "|downloadFromPMC.ipynb|download the pubmed text|\n",
    "|train_pmc_word2vec.ipynb| Train a word2vec model based on pubmed text|\n",
    "|keras_on_sra_data.ipynb| Train an entity recognition model using SRA meta data |\n",
    "\n",
    "|Data| Usage|\n",
    "|:--------------:|------:|\n",
    "|https://www.synapse.org/#!Synapse:syn11421651 | all SRS annotations|\n",
    "| https://www.synapse.org/#!Synapse:syn11421649 | all SRX annotations|\n",
    "|ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/PMC-ids.csv.gz|PUBMED ID conversions|\n",
    "\n",
    "### depending packages\n",
    "if u have anaconda, simply: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras gensim  nltk spacy tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### License\n",
    "This work is under Creative Commons Attribution license. This work is unpublished at the moment. Please attribute this work by citing the github page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatically update the notebook with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch\n",
    "Please ignore the bottom parts, it's just for my convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook README.ipynb to markdown\n",
      "[NbConvertApp] Writing 2486 bytes to README.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown README.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t      mergeEntities.ipynb\r\n",
      "NCIT_parsing.ipynb\t      pmc_word2_vec\r\n",
      "README.ipynb\t\t      pubmed\r\n",
      "README.md\t\t      read_embedding_matrix.ipynb\r\n",
      "Results\t\t\t      read_word_vector_matrix.ipynb\r\n",
      "Thesaurus.txt\t\t      semantic_count.csv\r\n",
      "Untitled9.ipynb\t\t      testPhraseMatcher.ipynb\r\n",
      "analyzeBioNLPEmbedding.ipynb  tmp.tsv\r\n",
      "analyzeSRAEntities.ipynb      tmp.txt\r\n",
      "downloadFromPMC.ipynb\t      tmpResults\r\n",
      "downloadPubmed.ipynb\t      tmpResults.xlsx\r\n",
      "download_wordvectors.ipynb    track_word_count_and_embedding_size.ipynb\r\n",
      "keras_on_sra_data.ipynb       train_pmc_word2vec.ipynb\r\n",
      "keras_on_sra_data_old.ipynb   train_pmc_word2vec.py\r\n",
      "merge.pmc\t\t      wikipedia-pubmed-and-PMC-w2v\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add README.ipynb README.md keras_on_sra_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master e778ba8] put most files in\r\n",
      " 2 files changed, 911 insertions(+), 731 deletions(-)\r\n",
      " rewrite keras_on_sra_data.ipynb (85%)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"put most files in \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: push.default is unset; its implicit value has changed in\n",
      "Git 2.0 from 'matching' to 'simple'. To squelch this message\n",
      "and maintain the traditional behavior, use:\n",
      "\n",
      "  git config --global push.default matching\n",
      "\n",
      "To squelch this message and adopt the new behavior now, use:\n",
      "\n",
      "  git config --global push.default simple\n",
      "\n",
      "When push.default is set to 'matching', git will push local branches\n",
      "to the remote branches that already exist with the same name.\n",
      "\n",
      "Since Git 2.0, Git defaults to the more conservative 'simple'\n",
      "behavior, which only pushes the current branch to the corresponding\n",
      "remote branch that 'git pull' uses to update the current branch.\n",
      "\n",
      "See 'git help config' and search for 'push.default' for further information.\n",
      "(the 'simple' mode was introduced in Git 1.7.11. Use the similar mode\n",
      "'current' instead of 'simple' if you sometimes use older versions of Git)\n",
      "\n",
      "Counting objects: 7, done.\n",
      "Delta compression using up to 96 threads.\n",
      "Compressing objects: 100% (7/7), done.\n",
      "Writing objects: 100% (7/7), 117.04 KiB | 0 bytes/s, done.\n",
      "Total 7 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 1 local object.\u001b[K\n",
      "To git@github.com:brianyiktaktsui/DEEP_NLP.git\n",
      "   e92c092..e778ba8  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git push "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### status \n",
    "retraining the word2vec models\n",
    "\n",
    "training the word2vec using the entire PMC now: \n",
    "train_pmc_word2vec.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ./Data/\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/PMC-ids.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -c ./Data/PMC-ids.csv.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
