{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import spacy \n",
    "import numpy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 s, sys: 7.6 s, total: 49.5 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#nlp=spacy.load('en_vectors_web_lg')\n",
    "%time nlp=spacy.load('./wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRecognizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            ##rever to word vector\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take SRS descripitions for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 7.64 s, total: 25.9 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = numpy.load('./model/classes.npy')\n",
    "%time model=load_model('./model/lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell type/tissue, Disease state, Genotype, Geographical Location, Species, Treatment and conditions\n"
     ]
    }
   ],
   "source": [
    "print (', '.join(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedAttrib='description'\n",
    "timeStamp='1535393121.334881'\n",
    "inPickleDir='./Data/validation_data/validation_{}.{}.pickle'.format(selectedAttrib,timeStamp)\n",
    "\n",
    "inTestStrFlatS=pd.read_pickle(inPickleDir).head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print ('\\n'.join(inTestStrFlatS.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get baseline empty state\n",
    " Since our model does not capture the empty state, we estimate the baseline neural net emission strength using the a single blank space. All the emission vectors with absolute sum difference < 0.01 with baseline emission were zeroed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_docs = list(nlp.pipe(' '))\n",
    "val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "\n",
    "emptyState=model.predict_proba(val_X)[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate NER score for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopWords=stopwords.words('english')\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraseMax=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTestStrS=inTestStrFlatS.str.split('[;.,]',expand=True).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTestStrS=inTestStrS.str.replace('\\s+',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inTestStrS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [00:05<00:00, 38.10it/s]\n"
     ]
    }
   ],
   "source": [
    "myML=[]\n",
    "myKeyL=[]\n",
    "for i_th,(key,sent) in enumerate(tqdm(inTestStrS.items(),total=len(inTestStrS))):\n",
    "    sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)## remove non alpha numeric \n",
    "    tokens=re.split(pattern=' ',string=sent)\n",
    "    tokens=list(filter(lambda token:(token!='') and (token not in stopWords)  ,tokens))\n",
    "    sent=' '.join(tokens)\n",
    "    ###keep track of each token\n",
    "    scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "    myNMax=min( [len(tokens),phraseMax])\n",
    "    for n_gram in range(1,myNMax+1):\n",
    "        grams=list(map(lambda L:\" \".join(L),list(ngrams(tokens,n_gram))))\n",
    "        val_docs = list(nlp.pipe(grams))\n",
    "        val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "        \n",
    "        predictM=model.predict_proba(val_X)\n",
    "        \n",
    "        tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "        empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "        moreThanTwoValToken_mask=(val_X!=0).sum(axis=1)>=2\n",
    "        tmpDf[empty_mask&moreThanTwoValToken_mask]=0\n",
    "        for i,gram in enumerate(tmpDf.index): #i: track the each token position\n",
    "            #for j,one_gram in enumerate(gram.split(' ')):\n",
    "            i_end=i+n_gram+1\n",
    "            textBefore=\" \".join(tokens[:i]) + ('' if i==0 else ' ')\n",
    "            start_char_pos=len(textBefore)\n",
    "            myKeyL.append(key+(i_th,sent,n_gram,i,i_end,gram,start_char_pos)) \n",
    "            myML.append(tmpDf.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf=pd.concat(myML,keys=myKeyL,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "outValidationSentDir='./Data/validation_data/validation_{}.{}.cleaned.txt'.format(selectedAttrib,timeStamp)\n",
    "with open(outValidationSentDir,'w') as f:\n",
    "    f.write('\\n'.join(mergedDf.index.get_level_values('orig_text').unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wc -l $PWD/./Data/validation_data/validation_description.1535393121.334881.cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/cellardata/users/btsui/DEEP_NLP/NLP_spacy/./Data/validation_data/validation_description.1535393121.334881.cleaned.txt\r\n"
     ]
    }
   ],
   "source": [
    "!echo  $PWD/./Data/validation_data/validation_description.1535393121.334881.cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf.index.names=['srs','attribute','n_sent']+['i_thSrs','orig_text','n','i','i_end','token','ith_char_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf_sub=mergedDf[mergedDf.index.get_level_values('n')>=n_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxS=mergedDf_sub.max(axis=1)#>0.3\n",
    "secondBestScoreS=mergedDf_sub.quantile(0.999,interpolation='lower',axis=1)\n",
    "scoreMargin_m=(maxS-secondBestScoreS)>0.1\n",
    "m_val=scoreMargin_m&(~mergedDf_sub.index.get_level_values('token').str.contains('[0-9 ]+ [0-9 ]+'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=pd.DataFrame({'predicted':mergedDf_sub[m_val].idxmax(axis=1),'score':mergedDf_sub[m_val].max(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/envs/deep_nlp_cpu/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "scoreSortedDf=tmpDf[m_val].sort_values(['orig_text','i','score'],ascending=False).reset_index(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all overlapping regions\n",
    "v=scoreSortedDf.copy()\n",
    "scoreSortedDf=scoreSortedDf.assign(OverlapGroup=(len(inTestStrS)*v.i_thSrs+ \n",
    "                                          (v.i_end - v.i.shift(-1)).shift().lt(0).cumsum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoreSortedDf[scoreSortedDf[('orig_text')].str.contains('Human dermal lymphatic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoreSortedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf=scoreSortedDf.sort_values(['OverlapGroup','score'],ascending=False).drop_duplicates(['OverlapGroup','predicted']\n",
    "                                                                                   ).sort_values('orig_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['token_len']=hitDf['token'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['recovered_txt']=hitDf.apply(\n",
    "    lambda tmpS2:tmpS2.loc['orig_text'][tmpS2.loc['ith_char_pos']:(tmpS2.loc['ith_char_pos']+tmpS2.loc['token_len'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srs</th>\n",
       "      <th>attribute</th>\n",
       "      <th>n_sent</th>\n",
       "      <th>i_thSrs</th>\n",
       "      <th>orig_text</th>\n",
       "      <th>n</th>\n",
       "      <th>i</th>\n",
       "      <th>i_end</th>\n",
       "      <th>token</th>\n",
       "      <th>ith_char_pos</th>\n",
       "      <th>predicted</th>\n",
       "      <th>score</th>\n",
       "      <th>OverlapGroup</th>\n",
       "      <th>token_len</th>\n",
       "      <th>recovered_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>ERS568448</td>\n",
       "      <td>description</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Instestinal Gastric Cancer stage T4N1M0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>stage T4N1M0</td>\n",
       "      <td>27</td>\n",
       "      <td>Disease state</td>\n",
       "      <td>0.591016</td>\n",
       "      <td>905</td>\n",
       "      <td>12</td>\n",
       "      <td>stage T4N1M0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            srs    attribute  n_sent  i_thSrs  \\\n",
       "1169  ERS568448  description       1        4   \n",
       "\n",
       "                                    orig_text  n  i  i_end         token  \\\n",
       "1169  Instestinal Gastric Cancer stage T4N1M0  2  3      6  stage T4N1M0   \n",
       "\n",
       "      ith_char_pos      predicted     score  OverlapGroup  token_len  \\\n",
       "1169            27  Disease state  0.591016           905         12   \n",
       "\n",
       "     recovered_txt  \n",
       "1169  stage T4N1M0  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitDf[hitDf.orig_text=='Instestinal Gastric Cancer stage T4N1M0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hitDf[hitDf['srs'].str.contains('SRS1965591')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is it extraction:  True\n"
     ]
    }
   ],
   "source": [
    "print ('is it extraction: ',(hitDf.recovered_txt==hitDf.token).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}\n",
    "\\\n",
    "Content contains input text, annotation has the labeled content, extras is for some extra columns that you want to show with each row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClassToId={}\n",
    "for myClass in le.classes_:\n",
    "    myClassToId[myClass]=nlp.vocab.strings.add(myClass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "classToSpacyId=pd.Series(myClassToId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDisplayHitDf=hitDf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDisplayHitDf['predicted_entity_id']=classToSpacyId.loc[inDisplayHitDf['predicted']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inDisplayHitDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inTestStrFlatS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inDisplayHitDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text_S=inDisplayHitDf.drop_duplicates(['srs','n_sent','orig_text']).set_index(['srs','n_sent'])['orig_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectedAttrib='description'\n",
    "#timeStamp='1535393121.334881'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_description.1535393121.334881.cleaned.txt\r\n",
      "validation_description.1535393121.334881.pickle\r\n",
      "validation_description.1535393121.334881.xlsx\r\n",
      "validation_description_1535393121.334881.txt\r\n",
      "validation_prediction_description.1535393121.334881.html\r\n"
     ]
    }
   ],
   "source": [
    "#!ls ./Data/validation_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf.to_pickle('./Data/validation_data/validataion_prediction_{}.{}.pickle'.format(selectedAttrib,timeStamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_textL=orig_text_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABLCAYAAABz9YPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAArBJREFUeJzt3aFqlmEYxvH7FedWFkQHDkQZw+aCuKqIJyFosO0UPASbWCxiswkegh6BNkUwDJsMJsKSYfJYLBY/xnh49r3X79cGT7jutD+8g02ttQIASHVu9AAAgJHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHOL3owTdNeVe1VVU0ra7dXLl3tPmqUnWl/9ISuPq9eGD2hm+3voxf0dbR+bfSErtrvg9ETurq4emX0hK4Op6PRE7pZX/8xekJX+7U9ekJXx1+/HLbWNha9m07y7zhWN2+0zcfPTzXsLPu29nD0hK52tub7C/XN0+PRE7p6f+/F6Ald/fr5bPSErh5sPRk9oatXa+9GT+jmzt3Xoyd09Wh6O3pCVwf3b31sre0ueuczGQAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANGm1tr/H0zTXlXt/f3xZlV96j1qoMtVdTh6RCdzvq3KfcvOfctrzrdVuW/ZXW+tbSx6tDCG/nk8TR9aa7unmnWGzfm+Od9W5b5l577lNefbqtyXwmcyACCaGAIAop00hl52WXF2zPm+Od9W5b5l577lNefbqtwX4UR/MwQAMDc+kwEA0cQQABBNDAEA0cQQABBNDAEA0f4Az7tyiUpEylQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)\n",
    "colors = {ent.upper():matplotlib.colors.to_hex(myColor)  for myColor,ent in zip(current_palette,classToSpacyId.index)}\n",
    "options = {'ents': classToSpacyId.index.str.upper(),\n",
    "           'colors': colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf_groupby=inDisplayHitDf.groupby('srs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srs         n_sent\n",
       "SRS463542   1                           0 hrs adipocyte differentiation\n",
       "ERS935021   2                                       0 subjected RNA seq\n",
       "SRS345378   1                            004 Cigarette Smoke Condensate\n",
       "SRS1719587  1         1 AML cells identify potential co operating mu...\n",
       "ERS040892   1         100 ng purified mRNA using poly A selection us...\n",
       "SRS1917013  4                                             2 probability\n",
       "SRS1409967  0                                     293T 3T3 Cell Mixture\n",
       "ERS1073743  8                                               4 FBS media\n",
       "            4                                               4 FBS media\n",
       "SRS696717   1           5 Vertebral column tissue including tail CME242\n",
       "SRS607088   1                                            5 globin locus\n",
       "SRS1269936  0         5 pooled animals mixed gender RNA harvested tr...\n",
       "ERS396144   12                                 AB5535 Sox21 R D systems\n",
       "            13                             AF3538 transcription factors\n",
       "ERS147858   1         After 10 minutes tissue removed diced 500 mM g...\n",
       "            2               After homogenization hepatocytes rinsed PBS\n",
       "ERS525834   0                  All samples used validation tool ENCODER\n",
       "SRS568315   0                        Ancient Hungarian Human sample IR1\n",
       "ERS396144   2                                       Ascl1 BD Pharmingen\n",
       "ERS742292   0                         Before antiviral agents treatment\n",
       "SRS1098269  2                C MYC KLF4 presence shp53 RNA lentiviruses\n",
       "SRS1917013  3                     C14 dating Poz 77332 1953BC 1880BC 68\n",
       "DRS012545   0         C57BL 6J mice fed 15weeks either normal diet R...\n",
       "SRS1345371  0         Case 1 Breast carcinoma neuroendocrine differe...\n",
       "ERS1073743  1                                 Cells cultured EBM2 media\n",
       "ERS565073   0         ChOP seq data aligned human hg19 genome BAM fi...\n",
       "SRS1007476  1                                  Collected upon resection\n",
       "DRS012545   3                    D06072701 high fat diet Research Diets\n",
       "            6                                     D07012601 6 weeks old\n",
       "SRS2126731  0         DNA damage induced etoposide hydroxyurea modul...\n",
       "                                            ...                        \n",
       "SRS713704   0                    day 5 FACS sorted Sca1 CD34 population\n",
       "ERS1101701  1         define mucosal ecological stability advancing ...\n",
       "SRS1098269  3         derived liver hepatoblastoma cell line HepG2 o...\n",
       "SRS1099347  0                                          first experiment\n",
       "SRS1337533  9                                              hnRNPK PTBP1\n",
       "            5                                        hnRNPK PTBP1 siRNA\n",
       "SRS1098269  0                              iPS cells introductions OCT4\n",
       "SRS584616   1               kept inside pyrographically decorated gourd\n",
       "SRS1337533  4                                     knockdown lncRNA LBCS\n",
       "SRS160413   5         miRNA extracted eight samples using AMbion mir...\n",
       "SRS1464851  0                              mixture 5 NA12878 95 NA12890\n",
       "DRS042036   0                               ntES cell line derived MEF3\n",
       "DRS042034   0                                ntES cell line derived TT2\n",
       "SRS002528   0                                      olfactory epithelium\n",
       "ERS391402   0                                  ovarian cancer cell line\n",
       "SRS1337533  7                          perform high throughput sequence\n",
       "SRS712720   0                            sample 1 lung cancer cell line\n",
       "ERS396144   7                                                    sc 197\n",
       "            9                                                  sc 30918\n",
       "            5                                                    sc 349\n",
       "ERS1075731  0                              sequence reads primer pool 1\n",
       "SRS160413   0         sequencing miRNA wild type diseased mouse hear...\n",
       "SRS1917013  6               sex male anthropological genetic assessment\n",
       "SRS401544   0                           source Induced endothelial cell\n",
       "SRS002638   0                                          source Input DNA\n",
       "SRS002556   0                        source Mouse embryonic fibroblasts\n",
       "SRS403905   0                                      source embryoid body\n",
       "ERS1073743  2                                         supplemented EGM2\n",
       "SRS000490   1                      targeted using Abcam antibody ab8580\n",
       "SRS2050342  0                                 transformed EBV infection\n",
       "Name: orig_text, Length: 165, dtype: object"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_textL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_L=[]\n",
    "hitDf_groupby=inDisplayHitDf.groupby('orig_text')\n",
    "for key,orig_text in orig_textL.items():#[10]\n",
    "    \n",
    "\n",
    "    inRecordDisplayDf=hitDf_groupby.get_group(orig_text)\n",
    "\n",
    "    doc=nlp(orig_text)\n",
    "\n",
    "    for _,tmpS in inRecordDisplayDf.iterrows():\n",
    "        EVENT=tmpS['predicted_entity_id']\n",
    "        entity=(EVENT,tmpS['i'],tmpS['i_end']-1)# this the correct one\n",
    "        #print (entity)\n",
    "        doc.ents+=(entity,)\n",
    "        \n",
    "        #optional\n",
    "        title=\"{}: sentence #{}  \".format(key[0],key[1])\n",
    "        doc.user_data['title']=title\n",
    "    doc_L.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inDisplayHitDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.displacy.render(doc, style='ent',jupyter=True,options=options)\n",
    "\n",
    "html=spacy.displacy.render(doc_L, style='ent',page=True,options=options)\n",
    "\n",
    "with open('./Data/validation_data/validation_prediction_{}.{}.html'.format(selectedAttrib,timeStamp),'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/validation_data/validation_prediction_description.1535393121.334881.html\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./Data/validation_data/validation_prediction_description.1535393121.334881.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/cellardata/users/btsui/DEEP_NLP/NLP_spacy/./Data/validation_data/validation_prediction_description.1535393121.334881.html\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD/./Data/validation_data/validation_prediction_description.1535393121.334881.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm ./Data/validation_data/validation_prediction_description.1535393121.334881.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerName='cell type'\n",
    "nlp.vocab.strings.add(nerName)\n",
    "EVENT = nlp.vocab.strings[nerName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"t cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use \n",
    "entity=(EVENT,0,len(u\"t cell\"))\n",
    "doc.ents+=(entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='ent',jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc\n",
    "    doc.ents += (entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToke.label=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents+=tuple(['cell type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span=doc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span.label='cell type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export to dataturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myF(tmpS1):\n",
    "    return {\"label\":[tmpS1.loc['predicted']],\"points\":{'start':tmpS1.loc['i'],\n",
    "     'end':tmpS1.loc['i_end']-1,'text':tmpS1.loc['token']},\n",
    "\n",
    "        }\n",
    "\n",
    "inAnnotDf=hitDf.copy()\n",
    "myL=[]\n",
    "for text,subDf in inAnnotDf.groupby(['orig_text']):\n",
    "    oneAnnotatedLine={\"content\":text,'annotation':list(subDf.apply(myF,axis=1)),\n",
    "                                \"extras\":None,\n",
    "        \"metadata\":{\"first_done_at\":1535058971000,\n",
    "                                     \"last_updated_at\":1535058971000,\"sec_taken\":0,\n",
    "                                \"last_updated_by\":\"EEOBDlEO48T6gzo0KHvT0IkqZnn2\"}}\n",
    "    myL+=[json.dumps(oneAnnotatedLine)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( './Data/validation.ngram.json.txt','w')as f:\n",
    "    f.write(\"\\n\".join(myL[:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Data/validation.ngram.json.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\",\".join(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneAnnotatedLine['annotation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDict={\"content\":\"cd players and tuners\",\"annotation\":[{\"label\":[\"Category\"],\"points\":[{\"start\":0,\"end\":1,\"text\":\"cd\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":3,\"end\":9,\"text\":\"players\"}]},{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}],\"extras\":{\"Name\":\"columnName\",\"Class\":\"ColumnValue\"}}\n",
    "exampleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"label\":[\"Category\"],\"points\":[{\"start\":15,\"end\":20,\"text\":\"tuners\"}]}\n",
    "#myL=[]\n",
    "#{\"content\":text,\"annotation\":}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inAnnotDf.iloc[:1].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['i_thSrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf['i_thSrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf.sort_values(['i_thSrs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNameL=list(VC.index.names)\n",
    "myNameL[-1]='Predicted_NE'\n",
    "VC.index.names=myNameL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for each n-gram, there is the same start site, use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.drop_duplicates(['orig_text','Predicted_NE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedDf\n",
    "#take on any length, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=(mergedDf>thresholdS).loc['ESCs WT replicate1 mRNA Mad2l2'].stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf.columns=['n','n-gram','attrib','passThreshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmpDf['n-gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf[tmpDf.passThreshold]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpS[tmpS].groupby(level=[1,2]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf2.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf=pd.concat(myML,keys=list(inTestStrS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_token_m=mergedDf.index.get_level_values(0).str.contains('^\\d+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf=mergedDf[~numeric_token_m].copy()#.loc[:,mergedDf.columns!='age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.800000\n",
    "mergedSubDf['undetected']=threshold\n",
    "#noAmbigM=(mergedSubDf>=threshold).sum(axis=1)>=1 #this mark screw up the confoudning boundary \n",
    "mergedSubDf.loc[:,'undetected']=threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf=mergedSubDf.idxmax(axis=1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf.index.names=['Freetext','Token']\n",
    "predDf['token_numeric']=predDf.index.get_level_values('Token').str.contains('^\\d+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel=pd.ExcelWriter('./Results/for_curation.xlsx')\n",
    "predDf[~predDf['token_numeric']].to_excel(excel)\n",
    "excel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf[~predDf['token_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Results/for_curation.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/./Results/for_curation.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf[~predDf['token_numeric']].to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm tmp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with duplicated states\n",
    "#emptyStat=np.array([0.42332533, 0.4360587 , 0.61020947, 0.42082471, 0.4110575 ,\n",
    "#       0.42533568, 0.47932082])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyState=mergedSubDf.groupby(mergedSubDf.columns.tolist(),as_index=False).size().sort_values().index[-1]\n",
    "#emptyStat=np.array([0.42332533, 0.4360587 , 0.61020947, 0.42082471, 0.4110575 ,\n",
    "#       0.42533568, 0.47932082])\n",
    "print (emptyState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noPredM=((mergedSubDf-emptyState).abs()<0.1).all(axis=1)\n",
    "mergedSubDf[(~noPredM)&(mergedSubDf>0.5).sum(axis=1)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good examples: HAP1 LMTK3-KO cells, stimulated with WNT3, replicate R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf.loc['HAP1 CCK4-KO cells, stimulated with RESV, replicate R1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedSubDf[].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=scoreDf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##among the ones with >0.5, take the ones that are unique\n",
    "sns.heatmap(data=(scoreDf>0.6).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=scoreDf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### among the ones with clear boundry, it can classify well. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "argue that it can salvage the data correctly. Among those sentences, \n",
    "\n",
    "take >0.5 as boundary, run top 10000 sentences \n",
    "\"\"\"\n",
    "scoreDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent='Human histone H3 di-methylated at lysine 4 (H3K4me2) in human blood CD4+ T cells, targeted using Abcam antibody ab7766'#inTestStrS.iloc[5]\n",
    "sent='RNA-seq of total RNA from Z/Edn2; Six3-Cre mouse retina\t'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "#print (sent)\n",
    "tokens=re.split(pattern=' ',string=sent)\n",
    "s=tokens\n",
    "#print (tokens)\n",
    "scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "#for n_gram in range(1,len(tokens)+1):\n",
    "#for n_gram in range(1,len(tokens)):\n",
    "n_gram=8\n",
    "grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "#print (grams)\n",
    "val_docs = list(nlp.pipe(grams))\n",
    "val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "tmpDf=pd.DataFrame(data=model.predict_proba(val_X),columns=le.classes_,index=grams)\n",
    "#tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "tmpDf[empty_mask]=0\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(3,2.5*4))\n",
    "sns.heatmap(tmpDf,annot=True,ax=ax,vmin=0,vmax=1.0,fmt='.2f',cbar=None)\n",
    "#ax.set_xticklabels([])\n",
    "\"\"\"break\n",
    "\n",
    "#each n gram only advange \n",
    "for i,gram in enumerate(tmpDf.index):# for ec\n",
    "    for j,one_gram in enumerate(gram.split(' ')):\n",
    "        scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each one, makes a prediction on the term, to see what it is supposed to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=re.sub(r'[^a-zA-Z0-9 ]+', ' ', sent)\n",
    "#print (sent)\n",
    "tokens=re.split(pattern=' ',string=sent)\n",
    "s=tokens\n",
    "#print (tokens)\n",
    "scoreDf=pd.DataFrame(columns=le.classes_,index=tokens).fillna(0)\n",
    "#for n_gram in range(1,len(tokens)+1):\n",
    "for n_gram in range(1,len(tokens)):\n",
    "    grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "    #print (grams)\n",
    "    val_docs = list(nlp.pipe(grams))\n",
    "    val_X=get_features(val_docs,max_length=model.input_shape[1])\n",
    "    predictM=model.predict_proba(val_X)\n",
    "\n",
    "    tmpDf=pd.DataFrame(data=predictM,columns=le.classes_,index=grams)\n",
    "    empty_mask=(tmpDf-emptyState).abs().sum(axis=1)<0.01\n",
    "    tmpDf[empty_mask]=0\n",
    "\n",
    "    \"\"\"\n",
    "    each n gram only advange \n",
    "    \"\"\"\n",
    "    for i,gram in enumerate(tmpDf.index):# for ec\n",
    "        for j,one_gram in enumerate(gram.split(' ')):\n",
    "            scoreDf.iloc[i+j]=numpy.maximum(scoreDf.iloc[i+j],(tmpDf.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.2\n",
    "scoreDf[scoreDf<=threshold]=0\n",
    "scoreDf['undetected']=threshold\n",
    "\n",
    "scoreDf.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scoreDf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scoreDf.T,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexS=srsS[srsS.index.get_level_values(1)=='sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSubSrsS1=srsS[srsS.str.contains('rna',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSubSrsS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_nlp_cpu]",
   "language": "python",
   "name": "conda-env-deep_nlp_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
