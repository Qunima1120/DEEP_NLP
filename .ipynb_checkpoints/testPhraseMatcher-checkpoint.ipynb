{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create entity \n",
    "\n",
    "How to do the maximal match\n",
    "\n",
    "1. tokenization phrase,  \n",
    "2. embedding\n",
    "3. do a classification on the text data\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_table=pd.read_pickle('./Data/NCIT_table.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the semantic type as entity types\n",
    "\n",
    ", synonyms , the pipeline accept theurapeutic or preventive procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_df['semantic type'].value_counts().to_csv('semantic_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantics_ignore=['Qualitative Concept']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df[NCIT_df['semantic type'].isin(semantics_ignore)].sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df\n",
    "\n",
    "NCIT_df=NCIT_table.reset_index().drop_duplicates(['code', 'concept name', 'parents', 'definition', 'semantic type',\n",
    "       'synonyms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare patterns\n",
    "\n",
    "input: synonyms\n",
    "\n",
    "\n",
    "\n",
    "output: tokenized synonyns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x2b8f5bb27a58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCIT_df.groupby('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of synonym: 388961\n",
      "# of multi mapped: 13603\n"
     ]
    }
   ],
   "source": [
    "synonymVC=NCIT_df['synonyms'].value_counts()\n",
    "print ('# of synonym:',len(synonymVC))\n",
    "print ('# of multi mapped:',(synonymVC>1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_df['synonym_word_len']=NCIT_df['synonyms'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_df['synonyms_lower']=NCIT_df['synonyms'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=NCIT_df.synonyms.isin(synonymVC.index[synonymVC==1])\n",
    "\n",
    "m_test=NCIT_df.synonyms_lower.isin(['tp53','t cell','cell'])\n",
    "\n",
    "m_text_length=NCIT_df.synonym_word_len>=3\n",
    "\n",
    "\"\"\"\n",
    "ascending=True: t, Cell \n",
    "\"\"\"\n",
    "ncit_sub_df=NCIT_df[m_text_length].drop_duplicates(\n",
    "    ['code','synonyms_lower']\n",
    "     ).sort_values('n_th synonym').drop_duplicates('synonyms_lower')#.sort_values('synonym_word_len',ascending=True)#.sort_values('n_th synonym',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df[NCIT_df.synonyms=='T-Cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131462/131462 [00:58<00:00, 2250.54it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "nerName='NCI'\n",
    "nlp.vocab.strings.add(nerName)\n",
    "EVENT = nlp.vocab.strings[nerName]\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc\n",
    "    doc.ents += (entity,)\n",
    "\"\"\"\n",
    "for each synonyms, make a pattern \n",
    "\"\"\"\n",
    "for _,tmpDf in  tqdm(ncit_sub_df.groupby('code')):\n",
    "    \n",
    "    #myEntityType=tmpDf['semantic type'].iloc[0]\n",
    "    term_name=tmpDf['synonyms'].iloc[0]\n",
    "    try:\n",
    "        myPatterns=[]\n",
    "        for  mySynonym in tmpDf['synonyms_lower'].tolist():\n",
    "            mySynonymSplitted=mySynonym.split(' ')\n",
    "            mySynonymSplitted=list(filter(lambda s: len(s)>0,mySynonymSplitted))\n",
    "            ##add tokens if it doens't exist\n",
    "            for myToken in mySynonymSplitted :\n",
    "                if myToken not in  nlp.vocab.strings:\n",
    "                    nlp.vocab.strings.add(myToken)              \n",
    "            myPatterns.append([{'LEMMA':myToken} for myToken in mySynonymSplitted])\n",
    "        args=[term_name,add_event_ent]+myPatterns\n",
    "        matcher.add(*args)\n",
    "        \n",
    "    except ValueError:\n",
    "        print ('failed ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize the srs data\n",
    "\n",
    "\n",
    "input: list of sentences, \n",
    "output: list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_pickle\n",
    "all_inS=pd.read_pickle(inS_dir)\n",
    "#srxS=pd.read_pickle('/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRX.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  test \n",
    "inS=all_inS.sample(n=100)#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=nlp.pipe(inS.tolist(),n_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  2.64it/s]\u001b[A\n",
      "2it [00:00,  4.08it/s]\u001b[A\n",
      "6it [00:00,  9.63it/s]\u001b[A\n",
      "10it [00:00, 12.16it/s]\u001b[A\n",
      "15it [00:00, 16.19it/s]\u001b[A\n",
      "20it [00:01, 18.20it/s]\u001b[A\n",
      "24it [00:01, 19.83it/s]\u001b[A\n",
      "28it [00:01, 21.19it/s]\u001b[A\n",
      "32it [00:01, 20.78it/s]\u001b[A\n",
      "37it [00:01, 22.22it/s]\u001b[A\n",
      "41it [00:01, 21.76it/s]\u001b[A\n",
      "44it [00:01, 22.07it/s]\u001b[A\n",
      "47it [00:02, 22.33it/s]\u001b[A\n",
      "50it [00:02, 22.56it/s]\u001b[A\n",
      "58it [00:02, 24.93it/s]\u001b[A\n",
      "64it [00:02, 26.17it/s]\u001b[A\n",
      "69it [00:02, 26.43it/s]\u001b[A\n",
      "75it [00:02, 27.20it/s]\u001b[A\n",
      "83it [00:02, 28.52it/s]\u001b[A\n",
      "89it [00:03, 29.51it/s]\u001b[A\n",
      "96it [00:03, 30.61it/s]\u001b[A\n",
      "100it [00:03, 31.39it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "matches_L=[]\n",
    "doc_L=[]\n",
    "for doc in tqdm(docs):\n",
    "    matches=matcher(doc)\n",
    "    matches_L.append(matches)\n",
    "    doc_L.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   Genome\n",
       "1              Genomic DNA\n",
       "2                      DNA\n",
       "3                  Extract\n",
       "4                     From\n",
       "5                Neisseria\n",
       "6    Neisseria gonorrhoeae\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpDf=pd.DataFrame(matches_L[1])\n",
    "tmpDf[0].apply(lambda s:nlp.vocab.strings[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genomic DNA extracted from Neisseria gonorrhoeae"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpDoc=doc_L[1]\n",
    "tmpDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a word2vect \n",
    "\n",
    "for the data, tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "specieS=sampleS[sampleS.index.get_level_values(1)=='SCIENTIFIC_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedSrs=specieS[specieS=='Mus musculus'].index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampleS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 256 ms, total: 2.52 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mouseSrsS=sampleS[sampleS.index.get_level_values(0).isin(selectedSrs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampleS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouseAttribVC=mouseSrsS.index.get_level_values(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#mouseSrsS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=mouseSrsS.groupby(level=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERS949109  SCIENTIFIC_NAME              Mus musculus\n",
       "           SUBMITTER_ID          IFMX1_1D-sc-2454762\n",
       "           strain                                   \n",
       "           sample_description    Lin_neg_bone marrow\n",
       "           ArrayExpress-Sex                   female\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouseSrsS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA          DNA\n",
      "Isolation          Isolation\n",
      "Kit          Kit\n",
      "Brain          Brain\n",
      "not          Negation\n",
      "determined          Determine\n",
      "hypothalamus          Hypothalamus\n",
      "whole          Whole\n",
      "genome          Genome\n",
      "sequence          Sequence\n",
      "strains          Strain\n",
      "Mus          Mouse\n",
      "Mus musculus          Mus musculus\n"
     ]
    }
   ],
   "source": [
    "inL=mouseSrsS.sample(n=1000).tolist()\n",
    "for i,send in enumerate(inL):\n",
    "    doc=nlp(send)\n",
    "    matches=matcher(doc)\n",
    "    for match in matches:\n",
    "        print (doc[match[1]:match[2]],'        ',nlp.vocab.strings[match[0]])\n",
    "    if i>10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCIT_df[NCIT_df.synonyms.isin(['Mus musculus','Mouse'])]['definition'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Screen 12 Lmnb2 Knock-out Embryonic Stem Cell Derived Macrophages stimulated with Salmonella"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Streptococcus pneumoniae\n",
       "Streptococcus pneumoniae\n",
       "SN20648-sc-2011-11-03T19:59:41Z-1101437\n",
       "\n",
       "\n",
       "not applicable\n",
       "Streptococcus pneumoniae\n",
       "Pathogen sample from Salmonella enterica\n",
       "Salmonella enterica subsp."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['percutaneous',\n",
       " 'coronary',\n",
       " 'intervention',\n",
       " '(pci)',\n",
       " 'for',\n",
       " 'high',\n",
       " 'risk',\n",
       " 'non-st',\n",
       " 'elevation',\n",
       " 'myocardial',\n",
       " 'infarction',\n",
       " 'or',\n",
       " 'unstable',\n",
       " 'angina']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySynonymSplitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'glioblastoma cell line TP53 t cell')\n",
    "matches = matcher(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "##word t probably map \n",
    "#lexem=doc.vocab['t']\n",
    "#lexem.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glioblastoma Glioblastoma\n",
      "cell Cell\n",
      "cell line Cell Line\n",
      "line Long Interspersed Element\n",
      "line Line\n",
      "TP53 TP53 Gene\n",
      "t cell T-Lymphocyte\n",
      "cell Cell\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print (doc[match[1]:match[2]],nlp.vocab.strings[match[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10878580257448234319, 5, 6)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP53 NCI\n",
      "t NCI\n",
      "cell NCI\n"
     ]
    }
   ],
   "source": [
    "for a in doc.ents:\n",
    "    print (a,a.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP53 t cell"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#EVENT = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP53 <function add_event_ent at 0x2b99bc527c80> [{'LEMMA': 'tp53'}]\n"
     ]
    }
   ],
   "source": [
    "def f(a,b,c):\n",
    "    print (a,b,c)\n",
    "f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'LEMMA': 'tp53'}]]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gene or Genome'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myEntityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8984921151469646652"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tp53, t cell)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFirst=NCIT_df.groupby('code')['synonyms'].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myVC=myFirst.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(myVC[myVC>1].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OrthsToPatterns(L):\n",
    "    L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inSynonymS.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleS=pd.read_pickle('/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher=Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp.make_doc(u'London is a big city in the United Kingdom.')\n",
    "print('Before', list(doc.ents))  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)))\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[u'GPE']\n",
    "doc.from_array(header, attr_array)\n",
    "print('After', list(doc.ents))  # [London]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge('NNP' if label else span.root.tag_, span.text, nlp.vocab.strings[label])\n",
    "    \n",
    "    matcher.add()\n",
    "matcher.add('GoogleNow', merge_phrases, [{ORTH: 'Google'}, {ORTH: 'Now'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_match(matcher, doc, id, matches):\n",
    "    print('Matched!', matches)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HelloWorld', on_match, [{'LOWER': 'hello'}, {'LOWER': 'world'}])\n",
    "matcher.add('GoogleMaps', on_match, [{'ORTH': 'Google'}, {'ORTH': 'Maps'}])\n",
    "doc = nlp(u'HELLO WORLD on Google Maps.')\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=doc.ents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/linguistic-features#on_match\n",
    "\n",
    "Adding on_match rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3492015959717556363, 5, 9), (3492015959717556363, 5, 10)]\n",
      "[(3492015959717556363, 5, 9), (3492015959717556363, 5, 10)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "##entity label, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent=doc.ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "using a function to define different entity in princ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "EVENT = nlp.vocab.strings['cell']\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    #print (matches)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc.ents += (entity,)\n",
    "\n",
    "    \n",
    "matcher.add('cell', add_event_ent,\n",
    "            [{'ORTH': 'cell'}])\n",
    "EVENT = nlp.vocab.strings['google']\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    #print (matches)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = (EVENT, start, end)\n",
    "    doc.ents += (entity,)\n",
    "matcher.add('GoogleIO', add_event_ent,\n",
    "            [{'ORTH': 'google'}])\n",
    "           # [{'ORTH': 'Google'}, {'ORTH': 'I'}, {'ORTH': '/'}, {'ORTH': 'O'}, {'IS_DIGIT': True}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"cell is google. 10 days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.vocab.strings['Therapeutic or Preventive Procedure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ent=doc.ents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings['cell']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
