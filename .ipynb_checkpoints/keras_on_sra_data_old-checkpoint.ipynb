{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threading do scale and ended up using most of the cores, \n",
    "\n",
    "parsing is slow still\n",
    "\n",
    "\n",
    "75232/75232 [==============================] - 144s 2ms/step - loss: 0.2044 - acc: 0.9252 - val_loss: 0.1596 - val_acc: 0.9404\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Need to improve the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This example shows how to use an LSTM sentiment classification model trained using Keras in spaCy. spaCy splits the document into sentences, and each sentence is classified using the LSTM. The scores for the sentences are then aggregated to give the document score. This kind of hierarchical model is quite difficult in \"pure\" Keras or Tensorflow, but it's very effective. The Keras example on this dataset performs quite poorly, because it cuts off the documents so that they're a fixed size. This hurts review accuracy a lot, because people often summarise their rating in the final sentence\n",
    "\n",
    "Prerequisites:\n",
    "spacy download en_vectors_web_lg\n",
    "pip install keras==2.0.9\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 788 ms, total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp=spacy.load('en_vectors_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.vocab.prune_vectors(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline in the data\n",
    "\"\"\"\n",
    "return: list of sentences, for each doc\n",
    "\"\"\"\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype='int32')\n",
    "\n",
    "\"\"\"\n",
    "input:list of docs\n",
    "Xs: doc by token matrix max length, each entry is the ID to the data\n",
    "\"\"\"\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            ##rever to word vector\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs\n",
    "\n",
    "#lstm_shape: {'nr_hidden': 64, 'max_length': 100, 'nr_class': 1},nr_hidden: reduce the number of activation?\n",
    "def train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "          lstm_shape, lstm_settings, lstm_optimizer, batch_size=100,\n",
    "          nb_epoch=5, by_sentence=True):\n",
    "    #print(\"Loading spaCy\")\n",
    "    #nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    #embedgigs is vocab.vectors.data\n",
    "    print ('embeddings:',embeddings)\n",
    "    print ('lstm_shape:',lstm_shape)\n",
    "    print ('lstm_settings:',lstm_settings)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "    print ('model:',model)\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "    dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
    "    model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
    "              nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "input: doc by tokens ID, in the form of setence. \n",
    "\n",
    "TimeDistributed: \n",
    "\n",
    "embeddings: \n",
    "\"\"\"\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    ###embedding layer, \n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True\n",
    "        )\n",
    "    )\n",
    "    ## TimeDistributed: wrapper applies a layer to every temporal slice of an input.\n",
    "    ## dense/ fully connected: a NN layer\n",
    "    ## dense, nr_hidden, reduce the number of units\n",
    "    ## \n",
    "    model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'],\n",
    "                                 recurrent_dropout=settings['dropout'],\n",
    "                                 dropout=settings['dropout'])))\n",
    "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
    "    #lr: learning rate, binary_crossentropy\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy',\n",
    "\t\t  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in SRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\n",
    "\n",
    "sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\n",
    "\n",
    "srsS=pd.read_pickle(inS_dir)\n",
    "\n",
    "\n",
    "technical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\n",
    "\n",
    "tmpDf=technical_meta_data_df.drop_duplicates('Sample')\n",
    "\n",
    "technical_meta_data_df=tmpDf.sample(n=tmpDf.shape[0],random_state=0)\n",
    "\n",
    "#technical_meta_data_df.head()\n",
    "\n",
    "n=5000\n",
    "subsampledSrs=technical_meta_data_df.groupby('Study').head(n=n)['Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srsS.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAttribs=['cell type','sex','age','genotype','treatment','SCIENTIFIC_NAME']\n",
    "m=srsS.index.get_level_values(0).isin(subsampledSrs.values)\n",
    "m1=srsS.index.get_level_values(1).isin(myAttribs)\n",
    "srsS_subS=srsS[m&m1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#srsS_subS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(myAttribs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=srsS_subS.reset_index()\n",
    "myDf.columns=['srs','attrib','sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTrainTestDf=myDf.sample(n=myDf.shape[0]).groupby('attrib').head(n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=model_selection.train_test_split(inTrainTestDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=None\n",
    "train_dir=None\n",
    "dev_dir=None\n",
    "is_runtime=False\n",
    "nr_hidden=64\n",
    "max_length=50#, # Shape\n",
    "dropout=0.5\n",
    "learn_rate=0.001#, # General NN config\n",
    "nb_epoch=1#\n",
    "batch_size=100\n",
    "nr_examples=-1  # Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_classes=len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "keras.utils.to_categorical(le.transform(train_df.attrib.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shape={'nr_hidden': 64, 'max_length': max_length, 'nr_class': nr_classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings: [[ 0.042175   0.34913   -0.32363   ...  0.22986   -0.30662    0.3878   ]\n",
      " [-0.63354   -0.1503    -0.36161   ...  0.26216   -0.12094    0.0038262]\n",
      " [-0.65276    0.23873   -0.23325   ... -0.42636    0.48578   -0.28969  ]\n",
      " ...\n",
      " [-0.51423   -0.23612   -0.035289  ... -0.20719   -0.60762    1.0162   ]\n",
      " [ 0.32269   -0.41217    0.044183  ...  0.29696    0.32911   -0.22805  ]\n",
      " [ 0.9066    -1.1523    -1.2483    ...  0.17064    0.56711    0.13522  ]]\n",
      "lstm_shape: {'nr_hidden': 64, 'max_length': 50, 'nr_class': 6}\n",
      "lstm_settings: {'dropout': 0.5, 'lr': 0.001}\n",
      "model: <keras.engine.sequential.Sequential object at 0x2afc6361b470>\n",
      "Parsing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/Data/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90957 samples, validate on 30293 samples\n",
      "Epoch 1/1\n",
      "90957/90957 [==============================] - 123s 1ms/step - loss: 0.1760 - acc: 0.9399 - val_loss: 0.1312 - val_acc: 0.9543\n",
      "CPU times: user 19min 36s, sys: 12min 40s, total: 32min 17s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm=train(train_df.sentence.tolist(),keras.utils.to_categorical(le.transform(train_df.attrib.tolist())),\n",
    "     test_df.sentence.tolist(),keras.utils.to_categorical(le.transform(test_df.attrib.tolist())),\n",
    "     {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': nr_classes},\n",
    "             {'dropout': dropout, 'lr': learn_rate},\n",
    "             {},\n",
    "             nb_epoch=nb_epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en_core_web_lg')\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=srsS.index.get_level_values(1)=='SCIENTIFIC_NAME'\n",
    "m2=srsS.values=='Mus musculus'\n",
    "scientifcS=srsS[m1&m2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.22 s, sys: 788 ms, total: 8.01 s\n",
      "Wall time: 8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "myAmbiguosAttribs=['TITLE']\n",
    "m=srsS.index.get_level_values(0).isin(subsampledSrs.values)\n",
    "m1=srsS.index.get_level_values(1).isin(myAmbiguosAttribs)\n",
    "m_species=srsS.index.get_level_values(0).isin(scientifcS.index.get_level_values(0))\n",
    "srsS_subS=srsS[m&m1&m_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230628"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(srsS_subS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "textDf_ambigous=srsS_subS.reset_index()\n",
    "textDf_ambigous.columns=['srs','attrib','sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDf=textDf_ambigous.sample(n=100)\n",
    "amb_text=subDf.sentence.str.replace('_',' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embeddings = get_embeddings(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_docs=nlp.pipe(amb_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=get_features(amb_docs,max_length=lstm_shape['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm.predict_proba(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreDf=pd.DataFrame( lstm.predict_proba(M),index=subDf.set_index(['srs','attrib','sentence']).index\n",
    "            ,columns=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/Data/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "scoreDf['predicted']=le.inverse_transform(lstm.predict_classes(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TITLE                  H57VWBGXY_E9_S153\n",
       "SCIENTIFIC_NAME             Mus musculus\n",
       "source_name        Mus musculus CD8 TILs\n",
       "cell type                       CD8 TILs\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srsS.loc['SRS1645209']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-3335b9dca393>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-3335b9dca393>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    myKeys=nlp.vocab.vectors.\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "myKeys=nlp.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.vocab.strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreDf.sort_values('cell type',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoreDf.idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le.transform(test_df.sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feed titles for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    #embedgigs is vocab.vectors.data\n",
    "    print ('embeddings:',embeddings)\n",
    "    print ('lstm_shape:',lstm_shape)\n",
    "    print ('lstm_settings:',lstm_settings)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "    print ('model:',model)\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "    dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
    "    model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
    "              nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ee25ef03c42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or direc\n",
    "libcudnn.so.7:\n",
    "\n",
    "conda install -c anaconda cudatoolkit\n",
    "conda install -c anaconda cudnn\n",
    "conda uninstall glibc -c asmeurer\n",
    "/lib64/libc.so.6:\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
