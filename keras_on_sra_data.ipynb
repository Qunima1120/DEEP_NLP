{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction and validation of bioNLP named entity recognition model using deep learning\n",
    "Problems: the embeddings I trained is worse than the one from google\n",
    "\n",
    "\n",
    "Attribution: \n",
    "    Some of the code are borrowed from the example code from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in SRA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\n",
    "\n",
    "sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\n",
    "\n",
    "srsS=pd.read_pickle(inS_dir)\n",
    "\n",
    "technical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 s, sys: 6.75 s, total: 53.7 s\n",
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#nlp=spacy.load('en_vectors_web_lg')\n",
    "nlp=spacy.load('./wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for model compilation and feature extraction\n",
    "\n",
    "Useful information for understanding the neural network: \n",
    "\n",
    "TimeDistributed: https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "\n",
    "lr stands for: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True\n",
    "        )\n",
    "    )\n",
    "    #the same dense layer is first applied extract the most useful info from embedding layers\n",
    "    #model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'],\n",
    "                                 recurrent_dropout=settings['dropout'],\n",
    "                                 dropout=settings['dropout'])))\n",
    "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy',\n",
    "\t\t  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            ##rever to word vector\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset sra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "srsWithText=srsS.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### take only intersection and than randomize the dataframe\n",
    "tmpDf=technical_meta_data_df.drop_duplicates('Sample')\n",
    "tmpInterDf=tmpDf[tmpDf.Sample.isin(srsWithText)]\n",
    "shuffledDf=tmpInterDf.sample(n=tmpInterDf.shape[0],random_state=0\n",
    "                                            )\n",
    "technical_meta_data_df_sub=shuffledDf.groupby('Study').head(n=max_sample_per_study_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('# of unique studies:',technical_meta_data_df_sub.Study.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset data based on entity types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample_per_study_n=100\n",
    "train_test_ratio=0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampledSrs=technical_meta_data_df.groupby('Study').head(n=max_sample_per_study_n)['Sample']\n",
    "myAttribs=['SCIENTIFIC_NAME','collected_by','cell_type','sex','age','growth_protocol','dev_stage','strain','genotype','disease','cell_line','treatment']\n",
    "m=srsS.index.get_level_values(0).isin(subsampledSrs.values)\n",
    "m1=srsS.index.get_level_values(1).isin(myAttribs)\n",
    "srsS_subS=srsS[m&m1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=srsS_subS.reset_index()\n",
    "myDf.columns=['srs','attrib','sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into training and  testing randomly by study levels\n",
    "\n",
    "It is split by study level to avoid overgeneralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106110\n"
     ]
    }
   ],
   "source": [
    "study_S=technical_meta_data_df_sub['Study'].drop_duplicates()\n",
    "print (len(study_S))\n",
    "myNStudies=len(study_S)\n",
    "train_n=int((myNStudies*train_test_ratio))\n",
    "train_studies=study_S.sample(n=train_n,random_state=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "technical_meta_data_df_sub['Train']=\\\n",
    "    technical_meta_data_df_sub['Study'].isin(train_studies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask=technical_meta_data_df_sub['Train']\n",
    "train_samples=technical_meta_data_df_sub['Sample'][train_mask].values\n",
    "test_samples=technical_meta_data_df_sub['Sample'][~train_mask].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### set training and testing within the dataframe\n",
    "#inTrainTestDf=myDf.sample(n=myDf.shape[0]).groupby('attrib').head(n=20000)\n",
    "train_df=myDf[myDf.srs.isin(train_samples)]\n",
    "test_df=myDf[myDf.srs.isin(test_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_hidden=64\n",
    "max_length=50#, # Shape\n",
    "dropout=0.5\n",
    "learn_rate=0.001#, # General NN config\n",
    "nb_epoch=1#\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the sklearn encoder going back and forth between classes in string format and integer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(myAttribs)\n",
    "nr_classes=len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shape={'nr_hidden': 64, 'max_length': max_length, 'nr_class': nr_classes}\n",
    "lstm_settings={'dropout': 0.5, 'lr': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nlp.vocab.vectors.data\n",
    "#embedgigs is vocab.vectors.data\n",
    "model = compile_lstm(embeddings, lstm_shape, lstm_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform list of freetexts into a matrix of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts=train_df.sentence.tolist()\n",
    "dev_texts=test_df.sentence.tolist()\n",
    "\n",
    "train_labels=keras.utils.to_categorical(\n",
    "    le.transform(train_df.attrib.values))\n",
    "dev_labels=keras.utils.to_categorical(le.transform(test_df.attrib.values))\n",
    "\n",
    "train_docs = list(nlp.pipe(train_texts))\n",
    "dev_docs = list(nlp.pipe(dev_texts))\n",
    "\n",
    "train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "dev_X = get_features(dev_docs, lstm_shape['max_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 530234 samples, validate on 262834 samples\n",
      "Epoch 1/1\n",
      "364100/530234 [===================>..........] - ETA: 2:35 - loss: 0.1441 - acc: 0.9576"
     ]
    }
   ],
   "source": [
    "lstm=model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
    "          nb_epoch=nb_epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_docs = list(nlp.pipe(dev_texts))\n",
    "val_X=get_features(val_docs,lstm_shape['max_length'])\n",
    "predictM=lstm.model.predict_proba(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf=pd.DataFrame(data=predictM,columns=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf.index=pd.MultiIndex.from_arrays([test_df.attrib.values,dev_texts],names=['entity','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probaDf=tmpDf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of model in validation cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "for myClass in le.classes_: \n",
    "    fpr,tpr,_=metrics.roc_curve((probaDf.entity==myClass),probaDf[myClass])    \n",
    "    ax.plot(fpr,tpr,label=\"{myClass}  (AUC: {AUC})\".format(myClass=myClass,AUC=str(metrics.auc(fpr,tpr))[:5]))\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for a sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"T cell is treated with LPS grow faster\"\n",
    "s=sent.split(' ')\n",
    "#def find_ngrams(input_list, n):\n",
    "#    return zip(*[input_list[i:] for i in range(n)])\n",
    "val_docs = list(nlp.pipe(s))\n",
    "val_X=get_features(val_docs,lstm_shape['max_length'])\n",
    "\n",
    "tmpDf=pd.DataFrame(data=lstm.model.predict_proba(val_X),columns=le.classes_,index=s)\n",
    "sns.heatmap(tmpDf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram=2\n",
    "grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "print (grams)\n",
    "val_docs = list(nlp.pipe(grams))\n",
    "val_X=get_features(val_docs,lstm_shape['max_length'])\n",
    "\n",
    "tmpDf=pd.DataFrame(data=lstm.model.predict_proba(val_X),columns=le.classes_,index=grams)\n",
    "ax=sns.heatmap(tmpDf,cbar_kws={'label': 'Emitted probability'},annot=True)\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('{} grams'.format(n_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X=get_features(val_docs,lstm_shape['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/Data/DEEP_NLP/NLP_spacy/keras_on_sra_data_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(lstm.model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat /proc/cpuinfo\n",
    "#https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy init-model en ./wikipedia-pubmed-and-PMC-w2v --vectors-loc ./Data/wikipedia-pubmed-and-PMC-w2v.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
