{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction and validation of bioNLP named entity recognition model using deep learning\n",
    "Problems: the embeddings I trained is worse than the one from google\n",
    "\n",
    "\n",
    "Attribution: \n",
    "    Some of the code are borrowed from the example code from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in SRA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "inS_dir='/cellar/users/btsui/Data/nrnb01_nobackup/METAMAP/allSRS.pickle'\n",
    "\n",
    "sra_dump_pickle_dir='/cellar/users/btsui/Data/SRA/DUMP/sra_dump.pickle'\n",
    "\n",
    "srsS=pd.read_pickle(inS_dir)\n",
    "%time srsS=pd.Series(data=srsS.values,index=pd.MultiIndex.from_arrays([srsS.index.get_level_values(0),\n",
    "                                                            srsS.index.get_level_values(1).str.lower()]) )\n",
    "technical_meta_data_df=pd.read_pickle(sra_dump_pickle_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.3 s, sys: 6.4 s, total: 57.7 s\n",
      "Wall time: 57.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#nlp=spacy.load('en_vectors_web_lg')\n",
    "nlp=spacy.load('./wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for model compilation and feature extraction\n",
    "\n",
    "Useful information for understanding the neural network: \n",
    "\n",
    "TimeDistributed: https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "\n",
    "lr stands for: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True\n",
    "        )\n",
    "    )\n",
    "    #the same dense layer is first applied extract the most useful info from embedding layers\n",
    "    #model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'],\n",
    "                                 recurrent_dropout=settings['dropout'],\n",
    "                                 dropout=settings['dropout'])))\n",
    "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy',\n",
    "\t\t  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            ##rever to word vector\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset sra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_length=10\n",
    "max_sample_per_study_n=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "srsWithText=srsS.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### take only intersection and than randomize the dataframe\n",
    "tmpDf=technical_meta_data_df.drop_duplicates('Sample')\n",
    "tmpInterDf=tmpDf[tmpDf.Sample.isin(srsWithText)]\n",
    "shuffledDf=tmpInterDf.sample(n=tmpInterDf.shape[0],random_state=0\n",
    "                                            )\n",
    "technical_meta_data_df_sub=shuffledDf.groupby('Study').head(n=max_sample_per_study_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique studies: 106110\n"
     ]
    }
   ],
   "source": [
    "print ('# of unique studies:',technical_meta_data_df_sub.Study.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset data based on entity types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampledSrs=technical_meta_data_df.groupby('Study').head(n=max_sample_per_study_n)['Sample']\n",
    "#,'dev_stage','phenotype'\n",
    "myAttribs=['organism','geo_loc_name','strain','sex','tissue','age','genotype','treatment',\n",
    "           'breed','body_product',\n",
    "           'disease','cell_line','ethnicity']\n",
    "#myAttribs=['SCIENTIFIC_NAME','biomaterial_provider','race','population_description','source_name','sex','age','strain','genotype','disease','treatment']\n",
    "m=srsS.index.get_level_values(0).isin(subsampledSrs.values)\n",
    "m1=srsS.index.get_level_values(1).isin(myAttribs)\n",
    "srsS_subS=srsS[m&m1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=srsS_subS.reset_index()\n",
    "myDf.columns=['srs','attrib','sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf['sentence']=myDf['sentence'].str.replace('_','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into training and  testing randomly by study levels\n",
    "\n",
    "It is split by study level to avoid overgeneralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106110\n"
     ]
    }
   ],
   "source": [
    "study_S=technical_meta_data_df_sub['Study'].drop_duplicates()\n",
    "print (len(study_S))\n",
    "myNStudies=len(study_S)\n",
    "train_n=int((myNStudies*train_test_ratio))\n",
    "train_studies=study_S.sample(n=train_n,random_state=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "technical_meta_data_df_sub['Train']=\\\n",
    "    technical_meta_data_df_sub['Study'].isin(train_studies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask=technical_meta_data_df_sub['Train']\n",
    "train_samples=technical_meta_data_df_sub['Sample'][train_mask].values\n",
    "test_samples=technical_meta_data_df_sub['Sample'][~train_mask].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824\n"
     ]
    }
   ],
   "source": [
    "### set training and testing within the dataframe\n",
    "#inTrainTestDf=myDf.sample(n=myDf.shape[0]).groupby('attrib').head(n=20000)\n",
    "cap_size=50000\n",
    "\n",
    "all_train_df=myDf[myDf.srs.isin(train_samples)]\n",
    "g=all_train_df.groupby('attrib')\n",
    "print (g.size().min())\n",
    "#train_df=g.head(g.size().min())\n",
    "train_df=all_train_df.sample(n=all_train_df.shape[0]).groupby('attrib').head(n=cap_size)\n",
    "all_test_df=myDf[myDf.srs.isin(test_samples)]\n",
    "test_df=all_test_df.sample(n=all_test_df.shape[0]).groupby('attrib').head(cap_size)\n",
    "#cap_size=g.size().min()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribVC_train=train_df.attrib.value_counts()#.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribVC_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tissue</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genotype</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strain</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_loc_name</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breed</th>\n",
       "      <td>30322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>organism</th>\n",
       "      <td>21191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease</th>\n",
       "      <td>14996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_line</th>\n",
       "      <td>9918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <td>5014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body_product</th>\n",
       "      <td>3824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              attrib\n",
       "age            50000\n",
       "sex            50000\n",
       "treatment      50000\n",
       "tissue         50000\n",
       "genotype       50000\n",
       "strain         50000\n",
       "geo_loc_name   50000\n",
       "breed          30322\n",
       "organism       21191\n",
       "disease        14996\n",
       "cell_line       9918\n",
       "ethnicity       5014\n",
       "body_product    3824"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribVC_train.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_hidden=64\n",
    "max_length=65#, # Shape\n",
    "dropout=0.5\n",
    "learn_rate=0.001#, # General NN config\n",
    "nb_epoch=1#\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the sklearn encoder going back and forth between classes in string format and integer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(myAttribs)\n",
    "nr_classes=len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shape={'nr_hidden': 64, 'max_length': max_length, 'nr_class': nr_classes}\n",
    "lstm_settings={'dropout': 0.5, 'lr': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nlp.vocab.vectors.data\n",
    "#embedgigs is vocab.vectors.data\n",
    "model = compile_lstm(embeddings, lstm_shape, lstm_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform list of freetexts into a matrix of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts=train_df.sentence.tolist()\n",
    "dev_texts=test_df.sentence.tolist()\n",
    "\n",
    "train_labels=keras.utils.to_categorical(\n",
    "    le.transform(train_df.attrib.values))\n",
    "dev_labels=keras.utils.to_categorical(le.transform(test_df.attrib.values))\n",
    "\n",
    "train_docs = list(nlp.pipe(train_texts))\n",
    "dev_docs = list(nlp.pipe(dev_texts))\n",
    "\n",
    "train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "dev_X = get_features(dev_docs, lstm_shape['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/btsui/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 435265 samples, validate on 249125 samples\n",
      "Epoch 1/1\n",
      "184500/435265 [===========>..................] - ETA: 5:08 - loss: 0.1403 - acc: 0.9616"
     ]
    }
   ],
   "source": [
    "lstm=model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
    "          nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "model.save('./model/lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('./model/classes.npy', le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_docs = list(nlp.pipe(dev_texts))\n",
    "val_X=get_features(val_docs,lstm_shape['max_length'])\n",
    "predictM=lstm.model.predict_proba(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probaDf_multI=pd.DataFrame(data=predictM,columns=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probaDf_multI.index=pd.MultiIndex.from_arrays([test_df.attrib.values,dev_texts],names=['entity','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probaDf=probaDf_multI.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show contingency tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minProbThreshold=0.2\n",
    "confidence_mask=probaDf_multI.max(axis=1)>=minProbThreshold\n",
    "tmpDf3=probaDf_multI[confidence_mask].idxmax(axis=1).reset_index(name='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continDf=tmpDf3.groupby(['predicted','entity']).size().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probaDf_multI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(12,5))\n",
    "#,cmap=\"Greens\"\n",
    "continDf=continDf.loc[attribVC_train.index,attribVC_train.index]\n",
    "sns.heatmap(ax=ax,data=continDf/continDf.sum(axis=0),annot=True,cbar_kws={'label':'% of samples'},center=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of model in validation cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "for myClass in le.classes_: \n",
    "    fpr,tpr,_=metrics.roc_curve((probaDf.entity==myClass),probaDf[myClass])    \n",
    "    ax.plot(fpr,tpr,label=\"{myClass}  (AUC: {AUC})\".format(myClass=myClass,AUC=str(metrics.auc(fpr,tpr))[:5]))\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdadasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-71b9e1639548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdadasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdadasd' is not defined"
     ]
    }
   ],
   "source": [
    "asdadasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for a sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sent=\"T cell is treated with LPS grow faster\"\n",
    "s=sent.split(' ')\n",
    "n_gram=2\n",
    "grams=list(map(lambda L:\" \".join(L),list(ngrams(s,n_gram))))\n",
    "print (grams)\n",
    "val_docs = list(nlp.pipe(grams))\n",
    "val_X=get_features(val_docs,lstm_shape['max_length'])\n",
    "tmpDf=pd.DataFrame(data=lstm.model.predict_proba(val_X),columns=le.classes_,index=grams)\n",
    "ax=sns.heatmap(tmpDf,cbar_kws={'label': 'Emitted probability'},annot=True)\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('{} grams'.format(n_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X=get_features(val_docs,lstm_shape['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD/Data/DEEP_NLP/NLP_spacy/keras_on_sra_data_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tmpS3=srsS[srsS.index.get_level_values(1)=='cur_land_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmpS3.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERS069057  TITLE                                Streptococcus pneumoniae\n",
       "           SCIENTIFIC_NAME                      Streptococcus pneumoniae\n",
       "           SUBMITTER_ID          SN20648-sc-2011-11-03T19:59:41Z-1101437\n",
       "           Strain                                                       \n",
       "           Sample Description                                           \n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no data: biomass,chem_mutagen\n",
    "srsS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
